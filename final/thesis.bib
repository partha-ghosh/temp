@Article{Zhang2022,
  author        = {Qihang Zhang and Zhenghao Peng and Bolei Zhou},
  title         = {Action-Conditioned Contrastive Policy Pretraining},
  year          = {2022},
  month         = apr,
  abstract      = {Deep visuomotor policy learning achieves promising results in control tasks such as robotic manipulation and autonomous driving, where the action is generated from the visual input by the neural policy. However, it requires a huge number of online interactions with the training environment, which limits its real-world application. Compared to the popular unsupervised feature learning for visual recognition, feature pretraining for visuomotor control tasks is much less explored. In this work, we aim to pretrain policy representations for driving tasks using hours-long uncurated YouTube videos. A new contrastive policy pretraining method is developed to learn action-conditioned features from video frames with action pseudo labels. Experiments show that the resulting action-conditioned features bring substantial improvements to the downstream reinforcement learning and imitation learning tasks, outperforming the weights pretrained from previous unsupervised learning methods. Code and models will be made publicly available.},
  archiveprefix = {arXiv},
  eprint        = {2204.02393},
  file          = {:Zhang2022 - Action Conditioned Contrastive Policy Pretraining.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.CV, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Zhang2022a,
  author        = {Jimuyang Zhang and Ruizhao Zhu and Eshed Ohn-Bar},
  title         = {SelfD: Self-Learning Large-Scale Driving Policies From the Web},
  year          = {2022},
  month         = apr,
  abstract      = {Effectively utilizing the vast amounts of ego-centric navigation data that is freely available on the internet can advance generalized intelligent systems, i.e., to robustly scale across perspectives, platforms, environmental conditions, scenarios, and geographical locations. However, it is difficult to directly leverage such large amounts of unlabeled and highly diverse data for complex 3D reasoning and planning tasks. Consequently, researchers have primarily focused on its use for various auxiliary pixel- and image-level computer vision tasks that do not consider an ultimate navigational objective. In this work, we introduce SelfD, a framework for learning scalable driving by utilizing large amounts of online monocular images. Our key idea is to leverage iterative semi-supervised training when learning imitative agents from unlabeled data. To handle unconstrained viewpoints, scenes, and camera parameters, we train an image-based model that directly learns to plan in the Bird's Eye View (BEV) space. Next, we use unlabeled data to augment the decision-making knowledge and robustness of an initially trained model via self-training. In particular, we propose a pseudo-labeling step which enables making full use of highly diverse demonstration data through "hypothetical" planning-based data augmentation. We employ a large dataset of publicly available YouTube videos to train SelfD and comprehensively analyze its generalization benefits across challenging navigation scenarios. Without requiring any additional data collection or annotation efforts, SelfD demonstrates consistent improvements (by up to 24%) in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA.},
  archiveprefix = {arXiv},
  eprint        = {2204.10320},
  file          = {:Zhang2022a - SelfD_ Self Learning Large Scale Driving Policies from the Web.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.CV, cs.AI, cs.RO},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Yadav2022,
  author        = {Karmesh Yadav and Ram Ramrakhya and Arjun Majumdar and Vincent-Pierre Berges and Sachit Kuhar and Dhruv Batra and Alexei Baevski and Oleksandr Maksymets},
  title         = {Offline Visual Representation Learning for Embodied Navigation},
  year          = {2022},
  month         = apr,
  abstract      = {How should we learn visual representations for embodied agents that must see and move? The status quo is tabula rasa in vivo, i.e. learning visual representations from scratch while also learning to move, potentially augmented with auxiliary tasks (e.g. predicting the action taken between two successive observations). In this paper, we show that an alternative 2-stage strategy is far more effective: (1) offline pretraining of visual representations with self-supervised learning (SSL) using large-scale pre-rendered images of indoor environments (Omnidata), and (2) online finetuning of visuomotor representations on specific tasks with image augmentations under long learning schedules. We call this method Offline Visual Representation Learning (OVRL). We conduct large-scale experiments - on 3 different 3D datasets (Gibson, HM3D, MP3D), 2 tasks (ImageNav, ObjectNav), and 2 policy learning algorithms (RL, IL) - and find that the OVRL representations lead to significant across-the-board improvements in state of art, on ImageNav from 29.2% to 54.2% (+25% absolute, 86% relative) and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28% relative). Importantly, both results were achieved by the same visual encoder generalizing to datasets that were not seen during pretraining. While the benefits of pretraining sometimes diminish (or entirely disappear) with long finetuning schedules, we find that OVRL's performance gains continue to increase (not decrease) as the agent is trained for 2 billion frames of experience.},
  archiveprefix = {arXiv},
  eprint        = {2204.13226},
  file          = {:Yadav2022 - Offline Visual Representation Learning for Embodied Navigation.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Prakash2021,
  author        = {Aditya Prakash and Kashyap Chitta and Andreas Geiger},
  title         = {Multi-Modal Fusion Transformer for End-to-End Autonomous Driving},
  year          = {2021},
  month         = apr,
  abstract      = {How should representations from complementary sensors be integrated for autonomous driving? Geometry-based sensor fusion has shown great promise for perception tasks such as object detection and motion forecasting. However, for the actual driving task, the global context of the 3D scene is key, e.g. a change in traffic light state can affect the behavior of a vehicle geometrically distant from that traffic light. Geometry alone may therefore be insufficient for effectively fusing representations in end-to-end driving models. In this work, we demonstrate that imitation learning policies based on existing sensor fusion methods under-perform in the presence of a high density of dynamic agents and complex scenarios, which require global contextual reasoning, such as handling traffic oncoming from multiple directions at uncontrolled intersections. Therefore, we propose TransFuser, a novel Multi-Modal Fusion Transformer, to integrate image and LiDAR representations using attention. We experimentally validate the efficacy of our approach in urban settings involving complex scenarios using the CARLA urban driving simulator. Our approach achieves state-of-the-art driving performance while reducing collisions by 76% compared to geometry-based fusion.},
  archiveprefix = {arXiv},
  eprint        = {2104.09224},
  file          = {:Prakash2021 - Multi Modal Fusion Transformer for End to End Autonomous Driving.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Yamada2022,
  author        = {Jun Yamada and Karl Pertsch and Anisha Gunjal and Joseph J. Lim},
  title         = {Task-Induced Representation Learning},
  year          = {2022},
  month         = apr,
  abstract      = {In this work, we evaluate the effectiveness of representation learning approaches for decision making in visually complex environments. Representation learning is essential for effective reinforcement learning (RL) from high-dimensional inputs. Unsupervised representation learning approaches based on reconstruction, prediction or contrastive learning have shown substantial learning efficiency gains. Yet, they have mostly been evaluated in clean laboratory or simulated settings. In contrast, real environments are visually complex and contain substantial amounts of clutter and distractors. Unsupervised representations will learn to model such distractors, potentially impairing the agent's learning efficiency. In contrast, an alternative class of approaches, which we call task-induced representation learning, leverages task information such as rewards or demonstrations from prior tasks to focus on task-relevant parts of the scene and ignore distractors. We investigate the effectiveness of unsupervised and task-induced representation learning approaches on four visually complex environments, from Distracting DMControl to the CARLA driving simulator. For both, RL and imitation learning, we find that representation learning generally improves sample efficiency on unseen tasks even in visually complex scenes and that task-induced representations can double learning efficiency compared to unsupervised alternatives. Code is available at https://clvrai.com/tarp.},
  archiveprefix = {arXiv},
  eprint        = {2204.11827},
  file          = {:Yamada2022 - Task Induced Representation Learning.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.LG, cs.AI, cs.RO},
  primaryclass  = {cs.LG},
  ranking       = {rank5},
}

@Article{Zhang2021,
  author        = {Jimuyang Zhang and Eshed Ohn-Bar},
  title         = {Learning by Watching},
  year          = {2021},
  month         = jun,
  abstract      = {When in a new situation or geographical location, human drivers have an extraordinary ability to watch others and learn maneuvers that they themselves may have never performed. In contrast, existing techniques for learning to drive preclude such a possibility as they assume direct access to an instrumented ego-vehicle with fully known observations and expert driver actions. However, such measurements cannot be directly accessed for the non-ego vehicles when learning by watching others. Therefore, in an application where data is regarded as a highly valuable asset, current approaches completely discard the vast portion of the training data that can be potentially obtained through indirect observation of surrounding vehicles. Motivated by this key insight, we propose the Learning by Watching (LbW) framework which enables learning a driving policy without requiring full knowledge of neither the state nor expert actions. To increase its data, i.e., with new perspectives and maneuvers, LbW makes use of the demonstrations of other vehicles in a given scene by (1) transforming the ego-vehicle's observations to their points of view, and (2) inferring their expert actions. Our LbW agent learns more robust driving policies while enabling data-efficient learning, including quick adaptation of the policy to rare and novel scenarios. In particular, LbW drives robustly even with a fraction of available driving data required by existing methods, achieving an average success rate of 92% on the original CARLA benchmark with only 30 minutes of total driving data and 82% with only 10 minutes.},
  archiveprefix = {arXiv},
  eprint        = {2106.05966},
  file          = {:Zhang2021 - Learning by Watching.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
  ranking       = {rank2},
  readstatus    = {read},
}

@Article{Chitta2021,
  author        = {Kashyap Chitta and Aditya Prakash and Andreas Geiger},
  title         = {NEAT: Neural Attention Fields for End-to-End Autonomous Driving},
  year          = {2021},
  month         = sep,
  abstract      = {Efficient reasoning about the semantic, spatial, and temporal structure of a scene is a crucial prerequisite for autonomous driving. We present NEural ATtention fields (NEAT), a novel representation that enables such reasoning for end-to-end imitation learning models. NEAT is a continuous function which maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and semantics, using intermediate attention maps to iteratively compress high-dimensional 2D image features into a compact representation. This allows our model to selectively attend to relevant regions in the input while ignoring information irrelevant to the driving task, effectively associating the images with the BEV representation. In a new evaluation setting involving adverse environmental conditions and challenging scenarios, NEAT outperforms several strong baselines and achieves driving scores on par with the privileged CARLA expert used to generate its training data. Furthermore, visualizing the attention maps for models with NEAT intermediate representations provides improved interpretability.},
  archiveprefix = {arXiv},
  eprint        = {2109.04456},
  file          = {:Chitta2021 - NEAT_ Neural Attention Fields for End to End Autonomous Driving.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
  ranking       = {rank1},
}

@Article{Bansal2018,
  author        = {Mayank Bansal and Alex Krizhevsky and Abhijit Ogale},
  title         = {ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst},
  year          = {2018},
  month         = dec,
  abstract      = {Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.},
  archiveprefix = {arXiv},
  eprint        = {1812.03079},
  file          = {:Bansal2018 - ChauffeurNet_ Learning to Drive by Imitating the Best and Synthesizing the Worst.pdf:PDF},
  keywords      = {cs.RO, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
  readstatus    = {read},
}

@Article{Behl2020,
  author        = {Aseem Behl and Kashyap Chitta and Aditya Prakash and Eshed Ohn-Bar and Andreas Geiger},
  title         = {Label Efficient Visual Abstractions for Autonomous Driving},
  year          = {2020},
  month         = may,
  abstract      = {It is well known that semantic segmentation can be used as an effective intermediate representation for learning driving policies. However, the task of street scene semantic segmentation requires expensive annotations. Furthermore, segmentation algorithms are often trained irrespective of the actual driving task, using auxiliary image-space loss functions which are not guaranteed to maximize driving metrics such as safety or distance traveled per intervention. In this work, we seek to quantify the impact of reducing segmentation annotation costs on learned behavior cloning agents. We analyze several segmentation-based intermediate representations. We use these visual abstractions to systematically study the trade-off between annotation efficiency and driving performance, i.e., the types of classes labeled, the number of image samples used to learn the visual abstraction model, and their granularity (e.g., object masks vs. 2D bounding boxes). Our analysis uncovers several practical insights into how segmentation-based visual abstractions can be exploited in a more label efficient manner. Surprisingly, we find that state-of-the-art driving performance can be achieved with orders of magnitude reduction in annotation cost. Beyond label efficiency, we find several additional training benefits when leveraging visual abstractions, such as a significant reduction in the variance of the learned policy when compared to state-of-the-art end-to-end driving models.},
  archiveprefix = {arXiv},
  eprint        = {2005.10091},
  file          = {:Behl2020 - Label Efficient Visual Abstractions for Autonomous Driving.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.RO},
  primaryclass  = {cs.CV},
  ranking       = {rank1},
}

@Article{Buehler2020,
  author        = {Andreas Bühler and Adrien Gaidon and Andrei Cramariuc and Rares Ambrus and Guy Rosman and Wolfram Burgard},
  title         = {Driving Through Ghosts: Behavioral Cloning with False Positives},
  year          = {2020},
  month         = aug,
  abstract      = {Safe autonomous driving requires robust detection of other traffic participants. However, robust does not mean perfect, and safe systems typically minimize missed detections at the expense of a higher false positive rate. This results in conservative and yet potentially dangerous behavior such as avoiding imaginary obstacles. In the context of behavioral cloning, perceptual errors at training time can lead to learning difficulties or wrong policies, as expert demonstrations might be inconsistent with the perceived world state. In this work, we propose a behavioral cloning approach that can safely leverage imperfect perception without being conservative. Our core contribution is a novel representation of perceptual uncertainty for learning to plan. We propose a new probabilistic birds-eye-view semantic grid to encode the noisy output of object perception systems. We then leverage expert demonstrations to learn an imitative driving policy using this probabilistic representation. Using the CARLA simulator, we show that our approach can safely overcome critical false positives that would otherwise lead to catastrophic failures or conservative behavior.},
  archiveprefix = {arXiv},
  eprint        = {2008.12969},
  file          = {:Buehler2020 - Driving through Ghosts_ Behavioral Cloning with False Positives.pdf:PDF},
  keywords      = {cs.CV, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
  readstatus    = {read},
}

@Article{Hawke2019,
  author        = {Jeffrey Hawke and Richard Shen and Corina Gurau and Siddharth Sharma and Daniele Reda and Nikolay Nikolov and Przemyslaw Mazur and Sean Micklethwaite and Nicolas Griffiths and Amar Shah and Alex Kendall},
  title         = {Urban Driving with Conditional Imitation Learning},
  year          = {2019},
  month         = nov,
  abstract      = {Hand-crafting generalised decision-making rules for real-world urban autonomous driving is hard. Alternatively, learning behaviour from easy-to-collect human driving demonstrations is appealing. Prior work has studied imitation learning (IL) for autonomous driving with a number of limitations. Examples include only performing lane-following rather than following a user-defined route, only using a single camera view or heavily cropped frames lacking state observability, only lateral (steering) control, but not longitudinal (speed) control and a lack of interaction with traffic. Importantly, the majority of such systems have been primarily evaluated in simulation - a simple domain, which lacks real-world complexities. Motivated by these challenges, we focus on learning representations of semantics, geometry and motion with computer vision for IL from human driving demonstrations. As our main contribution, we present an end-to-end conditional imitation learning approach, combining both lateral and longitudinal control on a real vehicle for following urban routes with simple traffic. We address inherent dataset bias by data balancing, training our final policy on approximately 30 hours of demonstrations gathered over six months. We evaluate our method on an autonomous vehicle by driving 35km of novel routes in European urban streets.},
  archiveprefix = {arXiv},
  eprint        = {1912.00177},
  file          = {:Hawke2019 - Urban Driving with Conditional Imitation Learning.pdf:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
}

@Article{Chen2019,
  author        = {Dian Chen and Brady Zhou and Vladlen Koltun and Philipp Krähenbühl},
  title         = {Learning by Cheating},
  year          = {2019},
  month         = dec,
  abstract      = {Vision-based urban driving is hard. The autonomous system needs to learn to perceive the world and act in it. We show that this challenging learning problem can be simplified by decomposing it into two stages. We first train an agent that has access to privileged information. This privileged agent cheats by observing the ground-truth layout of the environment and the positions of all traffic participants. In the second stage, the privileged agent acts as a teacher that trains a purely vision-based sensorimotor agent. The resulting sensorimotor agent does not have access to any privileged information and does not cheat. This two-stage training procedure is counter-intuitive at first, but has a number of important advantages that we analyze and empirically demonstrate. We use the presented approach to train a vision-based autonomous driving system that substantially outperforms the state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our approach achieves, for the first time, 100% success rate on all tasks in the original CARLA benchmark, sets a new record on the NoCrash benchmark, and reduces the frequency of infractions by an order of magnitude compared to the prior state of the art. For the video that summarizes this work, see https://youtu.be/u9ZCxxD-UUw},
  archiveprefix = {arXiv},
  eprint        = {1912.12294},
  file          = {:Chen2019 - Learning by Cheating.pdf:PDF},
  groups        = {E2E Driving},
  keywords      = {cs.RO, cs.AI, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Zhang2021a,
  author        = {Zhejun Zhang and Alexander Liniger and Dengxin Dai and Fisher Yu and Luc Van Gool},
  title         = {End-to-End Urban Driving by Imitating a Reinforcement Learning Coach},
  year          = {2021},
  month         = aug,
  abstract      = {End-to-end approaches to autonomous driving commonly rely on expert demonstrations. Although humans are good drivers, they are not good coaches for end-to-end algorithms that demand dense on-policy supervision. On the contrary, automated experts that leverage privileged information can efficiently generate large scale on-policy and off-policy demonstrations. However, existing automated experts for urban driving make heavy use of hand-crafted rules and perform suboptimally even on driving simulators, where ground-truth information is available. To address these issues, we train a reinforcement learning expert that maps bird's-eye view images to continuous low-level actions. While setting a new performance upper-bound on CARLA, our expert is also a better coach that provides informative supervision signals for imitation learning agents to learn from. Supervised by our reinforcement learning coach, a baseline end-to-end agent with monocular camera-input achieves expert-level performance. Our end-to-end agent achieves a 78% success rate while generalizing to a new town and new weather on the NoCrash-dense benchmark and state-of-the-art performance on the challenging public routes of the CARLA LeaderBoard.},
  archiveprefix = {arXiv},
  eprint        = {2108.08265},
  file          = {:Zhang2021a - End to End Urban Driving by Imitating a Reinforcement Learning Coach.pdf:PDF},
  keywords      = {cs.CV, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Caron2021,
  author        = {Mathilde Caron and Hugo Touvron and Ishan Misra and Hervé Jégou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  title         = {Emerging Properties in Self-Supervised Vision Transformers},
  year          = {2021},
  month         = apr,
  abstract      = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet in linear evaluation with ViT-Base.},
  archiveprefix = {arXiv},
  eprint        = {2104.14294},
  file          = {:Caron2021 - Emerging Properties in Self Supervised Vision Transformers.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Chen2020,
  author        = {Xinlei Chen and Kaiming He},
  title         = {Exploring Simple Siamese Representation Learning},
  year          = {2020},
  month         = nov,
  abstract      = {Siamese networks have become a common structure in various recent models for unsupervised visual representation learning. These models maximize the similarity between two augmentations of one image, subject to certain conditions for avoiding collapsing solutions. In this paper, we report surprising empirical results that simple Siamese networks can learn meaningful representations even using none of the following: (i) negative sample pairs, (ii) large batches, (iii) momentum encoders. Our experiments show that collapsing solutions do exist for the loss and structure, but a stop-gradient operation plays an essential role in preventing collapsing. We provide a hypothesis on the implication of stop-gradient, and further show proof-of-concept experiments verifying it. Our "SimSiam" method achieves competitive results on ImageNet and downstream tasks. We hope this simple baseline will motivate people to rethink the roles of Siamese architectures for unsupervised representation learning. Code will be made available.},
  archiveprefix = {arXiv},
  eprint        = {2011.10566},
  file          = {:Chen2020 - Exploring Simple Siamese Representation Learning.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
  printed       = {printed},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{He2019,
  author        = {Kaiming He and Haoqi Fan and Yuxin Wu and Saining Xie and Ross Girshick},
  title         = {Momentum Contrast for Unsupervised Visual Representation Learning},
  year          = {2019},
  month         = nov,
  abstract      = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  archiveprefix = {arXiv},
  eprint        = {1911.05722},
  file          = {:He2019 - Momentum Contrast for Unsupervised Visual Representation Learning.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Grill2020,
  author        = {Jean-Bastien Grill and Florian Strub and Florent Altché and Corentin Tallec and Pierre H. Richemond and Elena Buchatskaya and Carl Doersch and Bernardo Avila Pires and Zhaohan Daniel Guo and Mohammad Gheshlaghi Azar and Bilal Piot and Koray Kavukcuoglu and Rémi Munos and Michal Valko},
  title         = {Bootstrap your own latent: A new approach to self-supervised Learning},
  year          = {2020},
  month         = jun,
  abstract      = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches $74.3\%$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and $79.6\%$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archiveprefix = {arXiv},
  eprint        = {2006.07733},
  file          = {:Grill2020 - Bootstrap Your Own Latent_ a New Approach to Self Supervised Learning.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Chen2021,
  author        = {Xinlei Chen and Saining Xie and Kaiming He},
  title         = {An Empirical Study of Training Self-Supervised Vision Transformers},
  year          = {2021},
  month         = apr,
  abstract      = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: self-supervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other self-supervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
  archiveprefix = {arXiv},
  eprint        = {2104.02057},
  file          = {:Chen2021 - An Empirical Study of Training Self Supervised Vision Transformers.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Chen2020a,
  author        = {Ting Chen and Simon Kornblith and Mohammad Norouzi and Geoffrey Hinton},
  title         = {A Simple Framework for Contrastive Learning of Visual Representations},
  year          = {2020},
  month         = feb,
  abstract      = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archiveprefix = {arXiv},
  eprint        = {2002.05709},
  file          = {:Chen2020a - A Simple Framework for Contrastive Learning of Visual Representations.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
  printed       = {printed},
  ranking       = {rank5},
  readstatus    = {read},
}

@Article{Li2021,
  author        = {Chunyuan Li and Jianwei Yang and Pengchuan Zhang and Mei Gao and Bin Xiao and Xiyang Dai and Lu Yuan and Jianfeng Gao},
  title         = {Efficient Self-supervised Vision Transformers for Representation Learning},
  year          = {2021},
  month         = jun,
  abstract      = {This paper investigates two techniques for developing efficient self-supervised vision transformers (EsViT) for visual representation learning. First, we show through a comprehensive empirical study that multi-stage architectures with sparse self-attentions can significantly reduce modeling complexity but with a cost of losing the ability to capture fine-grained correspondences between image regions. Second, we propose a new pre-training task of region matching which allows the model to capture fine-grained region dependencies and as a result significantly improves the quality of the learned vision representations. Our results show that combining the two techniques, EsViT achieves 81.3% top-1 on the ImageNet linear probe evaluation, outperforming prior arts with around an order magnitude of higher throughput. When transferring to downstream linear classification tasks, EsViT outperforms its supervised counterpart on 17 out of 18 datasets. The code and models will be publicly available.},
  archiveprefix = {arXiv},
  eprint        = {2106.09785},
  file          = {:Li2021 - Efficient Self Supervised Vision Transformers for Representation Learning.pdf:PDF;:http\://arxiv.org/pdf/2106.09785v1:PDF},
  groups        = {SSL},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Zhou2019,
  author        = {Xingyi Zhou and Dequan Wang and Philipp Krähenbühl},
  title         = {Objects as Points},
  year          = {2019},
  month         = apr,
  abstract      = {Detection identifies objects as axis-aligned boxes in an image. Most successful object detectors enumerate a nearly exhaustive list of potential object locations and classify each. This is wasteful, inefficient, and requires additional post-processing. In this paper, we take a different approach. We model an object as a single point --- the center point of its bounding box. Our detector uses keypoint estimation to find center points and regresses to all other object properties, such as size, 3D location, orientation, and even pose. Our center point based approach, CenterNet, is end-to-end differentiable, simpler, faster, and more accurate than corresponding bounding box based detectors. CenterNet achieves the best speed-accuracy trade-off on the MS COCO dataset, with 28.1% AP at 142 FPS, 37.4% AP at 52 FPS, and 45.1% AP with multi-scale testing at 1.4 FPS. We use the same approach to estimate 3D bounding box in the KITTI benchmark and human pose on the COCO keypoint dataset. Our method performs competitively with sophisticated multi-stage methods and runs in real-time.},
  archiveprefix = {arXiv},
  eprint        = {1904.07850},
  file          = {:Zhou2019 - Objects As Points.pdf:PDF},
  groups        = {Misc},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Dosovitskiy2020,
  author        = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
  title         = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  year          = {2020},
  month         = oct,
  abstract      = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  archiveprefix = {arXiv},
  eprint        = {2010.11929},
  file          = {:Dosovitskiy2020 - An Image Is Worth 16x16 Words_ Transformers for Image Recognition at Scale.pdf:PDF},
  groups        = {Misc},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@InProceedings{Zhang2020,
  author    = {Zhang, Junzhe and Kumor, Daniel and Bareinboim, Elias},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Causal Imitation Learning With Unobserved Confounders},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {12263--12274},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:Zhang2020 - Causal Imitation Learning With Unobserved Confounders.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2020/file/8fdd149fcaa7058caccc9c4ad5b0d89a-Paper.pdf},
}

@Article{Haan2019,
  author        = {Pim de Haan and Dinesh Jayaraman and Sergey Levine},
  title         = {Causal Confusion in Imitation Learning},
  year          = {2019},
  month         = may,
  abstract      = {Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions---either environment interaction or expert queries---to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.},
  archiveprefix = {arXiv},
  eprint        = {1905.11979},
  file          = {:Haan2019 - Causal Confusion in Imitation Learning.pdf:PDF;:http\://arxiv.org/pdf/1905.11979v2:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Swamy2022,
  author        = {Gokul Swamy and Sanjiban Choudhury and J. Andrew Bagnell and Zhiwei Steven Wu},
  title         = {Causal Imitation Learning under Temporally Correlated Noise},
  year          = {2022},
  month         = feb,
  abstract      = {We develop algorithms for imitation learning from policy data that was corrupted by temporally correlated noise in expert actions. When noise affects multiple timesteps of recorded data, it can manifest as spurious correlations between states and actions that a learner might latch on to, leading to poor policy performance. To break up these spurious correlations, we apply modern variants of the instrumental variable regression (IVR) technique of econometrics, enabling us to recover the underlying policy without requiring access to an interactive expert. In particular, we present two techniques, one of a generative-modeling flavor (DoubIL) that can utilize access to a simulator, and one of a game-theoretic flavor (ResiduIL) that can be run entirely offline. We find both of our algorithms compare favorably to behavioral cloning on simulated control tasks.},
  archiveprefix = {arXiv},
  eprint        = {2202.01312},
  file          = {:Swamy2022 - Causal Imitation Learning under Temporally Correlated Noise.pdf:PDF;:http\://arxiv.org/pdf/2202.01312v1:PDF},
  keywords      = {cs.LG, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Codevilla2017,
  author        = {Felipe Codevilla and Matthias Müller and Antonio López and Vladlen Koltun and Alexey Dosovitskiy},
  title         = {End-to-end Driving via Conditional Imitation Learning},
  year          = {2017},
  month         = oct,
  abstract      = {Deep networks trained on demonstrations of human driving have learned to follow roads and avoid obstacles. However, driving policies trained via imitation learning cannot be controlled at test time. A vehicle trained end-to-end to imitate an expert cannot be guided to take a specific turn at an upcoming intersection. This limits the utility of such systems. We propose to condition imitation learning on high-level command input. At test time, the learned driving policy functions as a chauffeur that handles sensorimotor coordination but continues to respond to navigational commands. We evaluate different architectures for conditional imitation learning in vision-based driving. We conduct experiments in realistic three-dimensional simulations of urban driving and on a 1/5 scale robotic truck that is trained to drive in a residential area. Both systems drive based on visual input yet remain responsive to high-level navigational commands. The supplementary video can be viewed at https://youtu.be/cFtnflNe5fM},
  archiveprefix = {arXiv},
  eprint        = {1710.02410},
  file          = {:Codevilla2017 - End to End Driving Via Conditional Imitation Learning.pdf:PDF},
  groups        = {E2E Driving},
  keywords      = {cs.RO, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{Codevilla2019,
  author        = {Felipe Codevilla and Eder Santana and Antonio M. López and Adrien Gaidon},
  title         = {Exploring the Limitations of Behavior Cloning for Autonomous Driving},
  year          = {2019},
  month         = apr,
  abstract      = {Driving requires reacting to a wide variety of complex environment conditions and agent behaviors. Explicitly modeling each possible scenario is unrealistic. In contrast, imitation learning can, in theory, leverage data from large fleets of human-driven cars. Behavior cloning in particular has been successfully used to learn simple visuomotor policies end-to-end, but scaling to the full spectrum of driving behaviors remains an unsolved problem. In this paper, we propose a new benchmark to experimentally investigate the scalability and limitations of behavior cloning. We show that behavior cloning leads to state-of-the-art results, including in unseen environments, executing complex lateral and longitudinal maneuvers without these reactions being explicitly programmed. However, we confirm well-known limitations (due to dataset bias and overfitting), new generalization issues (due to dynamic objects and the lack of a causal model), and training instability requiring further research before behavior cloning can graduate to real-world driving. The code of the studied behavior cloning approaches can be found at https://github.com/felipecode/coiltraine .},
  archiveprefix = {arXiv},
  eprint        = {1904.08980},
  file          = {:Codevilla2019 - Exploring the Limitations of Behavior Cloning for Autonomous Driving.pdf:PDF;:http\://arxiv.org/pdf/1904.08980v1:PDF},
  groups        = {E2E Driving},
  keywords      = {cs.CV, cs.AI},
  primaryclass  = {cs.CV},
}

@Article{Zbontar2021,
  author        = {Jure Zbontar and Li Jing and Ishan Misra and Yann LeCun and Stéphane Deny},
  title         = {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  year          = {2021},
  month         = mar,
  abstract      = {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow's redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.},
  archiveprefix = {arXiv},
  eprint        = {2103.03230},
  file          = {:Zbontar2021 - Barlow Twins_ Self Supervised Learning Via Redundancy Reduction.pdf:PDF;:http\://arxiv.org/pdf/2103.03230v3:PDF},
  groups        = {SSL},
  keywords      = {cs.CV, cs.AI, cs.LG, q-bio.NC},
  primaryclass  = {cs.CV},
}

@Article{Bardes2021,
  author        = {Adrien Bardes and Jean Ponce and Yann LeCun},
  title         = {VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
  year          = {2021},
  month         = may,
  abstract      = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
  archiveprefix = {arXiv},
  eprint        = {2105.04906},
  file          = {:Bardes2021 - VICReg_ Variance Invariance Covariance Regularization for Self Supervised Learning.pdf:PDF},
  groups        = {SSL},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Florence2021,
  author        = {Pete Florence and Corey Lynch and Andy Zeng and Oscar Ramirez and Ayzaan Wahid and Laura Downs and Adrian Wong and Johnny Lee and Igor Mordatch and Jonathan Tompson},
  title         = {Implicit Behavioral Cloning},
  year          = {2021},
  month         = sep,
  abstract      = {We find that across a wide range of robot policy learning scenarios, treating supervised policy learning with an implicit model generally performs better, on average, than commonly used explicit models. We present extensive experiments on this finding, and we provide both intuitive insight and theoretical arguments distinguishing the properties of implicit models compared to their explicit counterparts, particularly with respect to approximating complex, potentially discontinuous and multi-valued (set-valued) functions. On robotic policy learning tasks we show that implicit behavioral cloning policies with energy-based models (EBM) often outperform common explicit (Mean Square Error, or Mixture Density) behavioral cloning policies, including on tasks with high-dimensional action spaces and visual image inputs. We find these policies provide competitive results or outperform state-of-the-art offline reinforcement learning methods on the challenging human-expert tasks from the D4RL benchmark suite, despite using no reward information. In the real world, robots with implicit policies can learn complex and remarkably subtle behaviors on contact-rich tasks from human demonstrations, including tasks with high combinatorial complexity and tasks requiring 1mm precision.},
  archiveprefix = {arXiv},
  eprint        = {2109.00137},
  file          = {:Florence2021 - Implicit Behavioral Cloning.pdf:PDF},
  groups        = {Misc},
  keywords      = {cs.RO, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{You2017,
  author        = {Yang You and Igor Gitman and Boris Ginsburg},
  title         = {Large Batch Training of Convolutional Networks},
  year          = {2017},
  month         = aug,
  abstract      = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
  archiveprefix = {arXiv},
  eprint        = {1708.03888},
  file          = {:You2017 - Large Batch Training of Convolutional Networks.pdf:PDF;:http\://arxiv.org/pdf/1708.03888v3:PDF},
  groups        = {Misc},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@InProceedings{9157137,
  author    = {E. Ohn-Bar and A. Prakash and A. Behl and K. Chitta and A. Geiger},
  booktitle = {2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Learning Situational Driving},
  year      = {2020},
  address   = {Los Alamitos, CA, USA},
  month     = {jun},
  pages     = {11293-11302},
  publisher = {IEEE Computer Society},
  doi       = {10.1109/CVPR42600.2020.01131},
  file      = {:Ohn-Bar_Learning_Situational_Driving_CVPR_2020_paper.pdf:PDF},
  keywords  = {task analysis;cloning;training;visualization;optimization;context modeling;cognition},
  url       = {https://doi.ieeecomputersociety.org/10.1109/CVPR42600.2020.01131},
}

@Article{Wang2020,
  author        = {Xinlong Wang and Rufeng Zhang and Chunhua Shen and Tao Kong and Lei Li},
  title         = {Dense Contrastive Learning for Self-Supervised Visual Pre-Training},
  year          = {2020},
  month         = nov,
  abstract      = {To date, most existing self-supervised learning methods are designed and optimized for image classification. These pre-trained models can be sub-optimal for dense prediction tasks due to the discrepancy between image-level prediction and pixel-level prediction. To fill this gap, we aim to design an effective, dense self-supervised learning method that directly works at the level of pixels (or local features) by taking into account the correspondence between local features. We present dense contrastive learning, which implements self-supervised learning by optimizing a pairwise contrastive (dis)similarity loss at the pixel level between two views of input images. Compared to the baseline method MoCo-v2, our method introduces negligible computation overhead (only <1% slower), but demonstrates consistently superior performance when transferring to downstream dense prediction tasks including object detection, semantic segmentation and instance segmentation; and outperforms the state-of-the-art methods by a large margin. Specifically, over the strong MoCo-v2 baseline, our method achieves significant improvements of 2.0% AP on PASCAL VOC object detection, 1.1% AP on COCO object detection, 0.9% AP on COCO instance segmentation, 3.0% mIoU on PASCAL VOC semantic segmentation and 1.8% mIoU on Cityscapes semantic segmentation. Code is available at: https://git.io/AdelaiDet},
  archiveprefix = {arXiv},
  eprint        = {2011.09157},
  file          = {:Wang2020 - Dense Contrastive Learning for Self Supervised Visual Pre Training.pdf:PDF;:http\://arxiv.org/pdf/2011.09157v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Jing2019,
  author        = {Longlong Jing and Yingli Tian},
  title         = {Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey},
  year          = {2019},
  month         = feb,
  abstract      = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  archiveprefix = {arXiv},
  eprint        = {1902.06162},
  file          = {:Jing2019 - Self Supervised Visual Feature Learning with Deep Neural Networks_ a Survey.pdf:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Godard2018,
  author        = {Clément Godard and Oisin Mac Aodha and Michael Firman and Gabriel Brostow},
  title         = {Digging Into Self-Supervised Monocular Depth Estimation},
  year          = {2018},
  month         = jun,
  abstract      = {Per-pixel ground-truth depth data is challenging to acquire at scale. To overcome this limitation, self-supervised learning has emerged as a promising alternative for training models to perform monocular depth estimation. In this paper, we propose a set of improvements, which together result in both quantitatively and qualitatively improved depth maps compared to competing self-supervised methods. Research on self-supervised monocular training usually explores increasingly complex architectures, loss functions, and image formation models, all of which have recently helped to close the gap with fully-supervised methods. We show that a surprisingly simple model, and associated design choices, lead to superior predictions. In particular, we propose (i) a minimum reprojection loss, designed to robustly handle occlusions, (ii) a full-resolution multi-scale sampling method that reduces visual artifacts, and (iii) an auto-masking loss to ignore training pixels that violate camera motion assumptions. We demonstrate the effectiveness of each component in isolation, and show high quality, state-of-the-art results on the KITTI benchmark.},
  archiveprefix = {arXiv},
  eprint        = {1806.01260},
  file          = {:Godard2018 - Digging into Self Supervised Monocular Depth Estimation.pdf:PDF},
  keywords      = {cs.CV, stat.ML},
  primaryclass  = {cs.CV},
}

@Article{Pathak2017,
  author        = {Deepak Pathak and Pulkit Agrawal and Alexei A. Efros and Trevor Darrell},
  title         = {Curiosity-driven Exploration by Self-supervised Prediction},
  year          = {2017},
  month         = may,
  abstract      = {In many real-world scenarios, rewards extrinsic to the agent are extremely sparse, or absent altogether. In such cases, curiosity can serve as an intrinsic reward signal to enable the agent to explore its environment and learn skills that might be useful later in its life. We formulate curiosity as the error in an agent's ability to predict the consequence of its own actions in a visual feature space learned by a self-supervised inverse dynamics model. Our formulation scales to high-dimensional continuous state spaces like images, bypasses the difficulties of directly predicting pixels, and, critically, ignores the aspects of the environment that cannot affect the agent. The proposed approach is evaluated in two environments: VizDoom and Super Mario Bros. Three broad settings are investigated: 1) sparse extrinsic reward, where curiosity allows for far fewer interactions with the environment to reach the goal; 2) exploration with no extrinsic reward, where curiosity pushes the agent to explore more efficiently; and 3) generalization to unseen scenarios (e.g. new levels of the same game) where the knowledge gained from earlier experience helps the agent explore new places much faster than starting from scratch. Demo video and code available at https://pathak22.github.io/noreward-rl/},
  archiveprefix = {arXiv},
  eprint        = {1705.05363},
  file          = {:Pathak2017 - Curiosity Driven Exploration by Self Supervised Prediction.pdf:PDF;:http\://arxiv.org/pdf/1705.05363v1:PDF},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Spencer2021,
  author        = {Jonathan Spencer and Sanjiban Choudhury and Arun Venkatraman and Brian Ziebart and J. Andrew Bagnell},
  title         = {Feedback in Imitation Learning: The Three Regimes of Covariate Shift},
  year          = {2021},
  month         = feb,
  abstract      = {Imitation learning practitioners have often noted that conditioning policies on previous actions leads to a dramatic divergence between "held out" error and performance of the learner in situ. Interactive approaches can provably address this divergence but require repeated querying of a demonstrator. Recent work identifies this divergence as stemming from a "causal confound" in predicting the current action, and seek to ablate causal aspects of current state using tools from causal inference. In this work, we argue instead that this divergence is simply another manifestation of covariate shift, exacerbated particularly by settings of feedback between decisions and input features. The learner often comes to rely on features that are strongly predictive of decisions, but are subject to strong covariate shift. Our work demonstrates a broad class of problems where this shift can be mitigated, both theoretically and practically, by taking advantage of a simulator but without any further querying of expert demonstration. We analyze existing benchmarks used to test imitation learning approaches and find that these benchmarks are realizable and simple and thus insufficient for capturing the harder regimes of error compounding seen in real-world decision making problems. We find, in a surprising contrast with previous literature, but consistent with our theory, that naive behavioral cloning provides excellent results. We detail the need for new standardized benchmarks that capture the phenomena seen in robotics problems.},
  archiveprefix = {arXiv},
  eprint        = {2102.02872},
  file          = {:Spencer2021 - Feedback in Imitation Learning_ the Three Regimes of Covariate Shift.pdf:PDF},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Ramesh2022,
  author        = {Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
  title         = {Hierarchical Text-Conditional Image Generation with CLIP Latents},
  year          = {2022},
  month         = apr,
  abstract      = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
  archiveprefix = {arXiv},
  eprint        = {2204.06125},
  file          = {:Ramesh2022 - Hierarchical Text Conditional Image Generation with CLIP Latents.pdf:PDF;:http\://arxiv.org/pdf/2204.06125v1:PDF},
  groups        = {Misc, SSL},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Radford2021,
  author        = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  title         = {Learning Transferable Visual Models From Natural Language Supervision},
  year          = {2021},
  month         = feb,
  abstract      = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  eprint        = {2103.00020},
  file          = {:Radford2021 - Learning Transferable Visual Models from Natural Language Supervision.pdf:PDF},
  groups        = {Misc, SSL},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@InProceedings{8578879,
  author    = {Cipolla, Roberto and Gal, Yarin and Kendall, Alex},
  booktitle = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  title     = {Multi-task Learning Using Uncertainty to Weigh Losses for Scene Geometry and Semantics},
  year      = {2018},
  pages     = {7482-7491},
  doi       = {10.1109/CVPR.2018.00781},
  file      = {:Kendall_Multi-Task_Learning_Using_CVPR_2018_paper.pdf:PDF},
}

@Article{Chitta2022,
  author        = {Kashyap Chitta and Aditya Prakash and Bernhard Jaeger and Zehao Yu and Katrin Renz and Andreas Geiger},
  title         = {TransFuser: Imitation with Transformer-Based Sensor Fusion for Autonomous Driving},
  year          = {2022},
  month         = may,
  abstract      = {How should we integrate representations from complementary sensors for autonomous driving? Geometry-based fusion has shown promise for perception (e.g. object detection, motion forecasting). However, in the context of end-to-end driving, we find that imitation learning based on existing sensor fusion methods underperforms in complex driving scenarios with a high density of dynamic agents. Therefore, we propose TransFuser, a mechanism to integrate image and LiDAR representations using self-attention. Our approach uses transformer modules at multiple resolutions to fuse perspective view and bird's eye view feature maps. We experimentally validate its efficacy on a challenging new benchmark with long routes and dense traffic, as well as the official leaderboard of the CARLA urban driving simulator. At the time of submission, TransFuser outperforms all prior work on the CARLA leaderboard in terms of driving score by a large margin. Compared to geometry-based fusion, TransFuser reduces the average collisions per kilometer by 48%.},
  archiveprefix = {arXiv},
  eprint        = {2205.15997},
  file          = {:Chitta2022 - TransFuser_ Imitation with Transformer Based Sensor Fusion for Autonomous Driving.pdf:PDF;:http\://arxiv.org/pdf/2205.15997v1:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Wang2022,
  author        = {Yanwei Wang and Ching-Yun Ko},
  title         = {Visual Pre-training for Navigation: What Can We Learn from Noise?},
  year          = {2022},
  month         = jun,
  abstract      = {A powerful paradigm for sensorimotor control is to predict actions from observations directly. Training such an end-to-end system allows representations that are useful for the downstream tasks to emerge automatically. In visual navigation, an agent can learn to navigate without any manual designs by correlating how its views change with the actions being taken. However, the lack of inductive bias makes this system data-inefficient and impractical in scenarios like search and rescue, where interacting with the environment to collect data is costly. We hypothesize a sufficient representation of the current view and the goal view for a navigation policy can be learned by predicting the location and size of a crop of the current view that corresponds to the goal. We further show that training such random crop prediction in a self-supervised fashion purely on random noise images transfers well to natural home images. The learned representation can then be bootstrapped to learn a navigation policy efficiently with little interaction data. Code is available at https://github.com/yanweiw/noise2ptz.},
  archiveprefix = {arXiv},
  eprint        = {2207.00052},
  file          = {:Wang2022 - Visual Pre Training for Navigation_ What Can We Learn from Noise_.pdf:PDF},
  groups        = {SSL, E2E Driving},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
  ranking       = {rank5},
}

@Article{Wang2017,
  author        = {Sen Wang and Ronald Clark and Hongkai Wen and Niki Trigoni},
  title         = {DeepVO: Towards End-to-End Visual Odometry with Deep Recurrent Convolutional Neural Networks},
  year          = {2017},
  month         = sep,
  abstract      = {This paper studies monocular visual odometry (VO) problem. Most of existing VO algorithms are developed under a standard pipeline including feature extraction, feature matching, motion estimation, local optimisation, etc. Although some of them have demonstrated superior performance, they usually need to be carefully designed and specifically fine-tuned to work well in different environments. Some prior knowledge is also required to recover an absolute scale for monocular VO. This paper presents a novel end-to-end framework for monocular VO by using deep Recurrent Convolutional Neural Networks (RCNNs). Since it is trained and deployed in an end-to-end manner, it infers poses directly from a sequence of raw RGB images (videos) without adopting any module in the conventional VO pipeline. Based on the RCNNs, it not only automatically learns effective feature representation for the VO problem through Convolutional Neural Networks, but also implicitly models sequential dynamics and relations using deep Recurrent Neural Networks. Extensive experiments on the KITTI VO dataset show competitive performance to state-of-the-art methods, verifying that the end-to-end Deep Learning technique can be a viable complement to the traditional VO systems.},
  archiveprefix = {arXiv},
  doi           = {10.1109/ICRA.2017.7989236},
  eprint        = {1709.08429},
  file          = {:Wang2017 - DeepVO_ Towards End to End Visual Odometry with Deep Recurrent Convolutional Neural Networks.pdf:PDF;:http\://arxiv.org/pdf/1709.08429v1:PDF},
  keywords      = {cs.CV, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Zhai2019,
  author        = {Guangyao Zhai and Liang Liu and Linjian Zhang and Yong Liu},
  title         = {PoseConvGRU: A Monocular Approach for Visual Ego-motion Estimation by Learning},
  year          = {2019},
  month         = jun,
  abstract      = {While many visual ego-motion algorithm variants have been proposed in the past decade, learning based ego-motion estimation methods have seen an increasing attention because of its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of fully trainable visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to map directly from input image pairs to an estimate of ego-motion (parameterized as 6-DoF transformation matrices). We introduce a novel two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU, with an explicit sequence pose estimation loss to achieve this. The feature-encoding module encodes the short-term motion feature in an image pair, while the memory-propagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6 channels tensor for module-1 to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through a stacked ConvGRU module to generate the relative transformation pose of each image pair. We also augment the training data by randomly skipping frames to simulate the velocity variation which results in a better performance in turning and high-velocity situations. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark. The experiments show a competitive performance of the proposed method to the geometric method and encourage further exploration of learning based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results.},
  archiveprefix = {arXiv},
  eprint        = {1906.08095},
  file          = {:Zhai2019 - PoseConvGRU_ a Monocular Approach for Visual Ego Motion Estimation by Learning.pdf:PDF},
  keywords      = {cs.CV, cs.LG, eess.IV},
  primaryclass  = {cs.CV},
}

@Article{Li2017,
  author        = {Ruihao Li and Sen Wang and Zhiqiang Long and Dongbing Gu},
  title         = {UnDeepVO: Monocular Visual Odometry through Unsupervised Deep Learning},
  year          = {2017},
  month         = sep,
  abstract      = {We propose a novel monocular visual odometry (VO) system called UnDeepVO in this paper. UnDeepVO is able to estimate the 6-DoF pose of a monocular camera and the depth of its view by using deep neural networks. There are two salient features of the proposed UnDeepVO: one is the unsupervised deep learning scheme, and the other is the absolute scale recovery. Specifically, we train UnDeepVO by using stereo image pairs to recover the scale but test it by using consecutive monocular images. Thus, UnDeepVO is a monocular system. The loss function defined for training the networks is based on spatial and temporal dense information. A system overview is shown in Fig. 1. The experiments on KITTI dataset show our UnDeepVO achieves good performance in terms of pose accuracy.},
  archiveprefix = {arXiv},
  eprint        = {1709.06841},
  file          = {:http\://arxiv.org/pdf/1709.06841v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Xie2019,
  author        = {Qizhe Xie and Minh-Thang Luong and Eduard Hovy and Quoc V. Le},
  title         = {Self-training with Noisy Student improves ImageNet classification},
  year          = {2019},
  month         = nov,
  abstract      = {We present Noisy Student Training, a semi-supervised learning approach that works well even when labeled data is abundant. Noisy Student Training achieves 88.4% top-1 accuracy on ImageNet, which is 2.0% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. Noisy Student Training extends the idea of self-training and distillation with the use of equal-or-larger student models and noise added to the student during learning. On ImageNet, we first train an EfficientNet model on labeled images and use it as a teacher to generate pseudo labels for 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the learning of the student, we inject noise such as dropout, stochastic depth, and data augmentation via RandAugment to the student so that the student generalizes better than the teacher. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet. Code is available at https://github.com/google-research/noisystudent.},
  archiveprefix = {arXiv},
  eprint        = {1911.04252},
  file          = {:Xie2019 - Self Training with Noisy Student Improves ImageNet Classification.pdf:PDF},
  keywords      = {cs.LG, cs.CV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Mueller2018,
  author        = {Matthias Müller and Alexey Dosovitskiy and Bernard Ghanem and Vladlen Koltun},
  title         = {Driving Policy Transfer via Modularity and Abstraction},
  year          = {2018},
  month         = apr,
  abstract      = {End-to-end approaches to autonomous driving have high sample complexity and are difficult to scale to realistic urban driving. Simulation can help end-to-end driving systems by providing a cheap, safe, and diverse training environment. Yet training driving policies in simulation brings up the problem of transferring such policies to the real world. We present an approach to transferring driving policies from simulation to reality via modularity and abstraction. Our approach is inspired by classic driving systems and aims to combine the benefits of modular architectures and end-to-end deep learning approaches. The key idea is to encapsulate the driving policy such that it is not directly exposed to raw perceptual input or low-level vehicle dynamics. We evaluate the presented approach in simulated urban environments and in the real world. In particular, we transfer a driving policy trained in simulation to a 1/5-scale robotic truck that is deployed in a variety of conditions, with no finetuning, on two continents. The supplementary video can be viewed at https://youtu.be/BrMDJqI6H5U},
  archiveprefix = {arXiv},
  eprint        = {1804.09364},
  file          = {:http\://arxiv.org/pdf/1804.09364v3:PDF},
  groups        = {Misc},
  keywords      = {cs.RO, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
}

@Book{10.2307/j.ctt183ph6v,
  author    = {RICHARD BELLMAN},
  publisher = {Princeton University Press},
  title     = {Adaptive Control Processes: A Guided Tour},
  year      = {1961},
  isbn      = {9780691079011},
  abstract  = {The book description for "Adaptive Control Processes" is currently unavailable.},
  url       = {http://www.jstor.org/stable/j.ctt183ph6v},
  urldate   = {2022-09-10},
}

@Article{Fischer2015,
  author        = {Philipp Fischer and Alexey Dosovitskiy and Eddy Ilg and Philip Häusser and Caner Hazırbaş and Vladimir Golkov and Patrick van der Smagt and Daniel Cremers and Thomas Brox},
  title         = {FlowNet: Learning Optical Flow with Convolutional Networks},
  year          = {2015},
  month         = apr,
  abstract      = {Convolutional neural networks (CNNs) have recently been very successful in a variety of computer vision tasks, especially on those linked to recognition. Optical flow estimation has not been among the tasks where CNNs were successful. In this paper we construct appropriate CNNs which are capable of solving the optical flow estimation problem as a supervised learning task. We propose and compare two architectures: a generic architecture and another one including a layer that correlates feature vectors at different image locations. Since existing ground truth data sets are not sufficiently large to train a CNN, we generate a synthetic Flying Chairs dataset. We show that networks trained on this unrealistic data still generalize very well to existing datasets such as Sintel and KITTI, achieving competitive accuracy at frame rates of 5 to 10 fps.},
  archiveprefix = {arXiv},
  eprint        = {1504.06852},
  file          = {:http\://arxiv.org/pdf/1504.06852v2:PDF},
  keywords      = {cs.CV, cs.LG, I.2.6; I.4.8},
  primaryclass  = {cs.CV},
}

@Article{Cho2014,
  author        = {Kyunghyun Cho and Bart van Merrienboer and Caglar Gulcehre and Dzmitry Bahdanau and Fethi Bougares and Holger Schwenk and Yoshua Bengio},
  title         = {Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation},
  year          = {2014},
  month         = jun,
  abstract      = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  archiveprefix = {arXiv},
  eprint        = {1406.1078},
  file          = {:http\://arxiv.org/pdf/1406.1078v3:PDF},
  keywords      = {cs.CL, cs.LG, cs.NE, stat.ML},
  primaryclass  = {cs.CL},
}

@Article{Filos2020,
  author        = {Angelos Filos and Panagiotis Tigas and Rowan McAllister and Nicholas Rhinehart and Sergey Levine and Yarin Gal},
  title         = {Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution Shifts?},
  year          = {2020},
  month         = jun,
  abstract      = {Out-of-training-distribution (OOD) scenarios are a common challenge of learning agents at deployment, typically leading to arbitrary deductions and poorly-informed decisions. In principle, detection of and adaptation to OOD scenes can mitigate their adverse effects. In this paper, we highlight the limitations of current approaches to novel driving scenes and propose an epistemic uncertainty-aware planning method, called \emph{robust imitative planning} (RIP). Our method can detect and recover from some distribution shifts, reducing the overconfident and catastrophic extrapolations in OOD scenes. If the model's uncertainty is too great to suggest a safe course of action, the model can instead query the expert driver for feedback, enabling sample-efficient online adaptation, a variant of our method we term \emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform current state-of-the-art approaches in the nuScenes \emph{prediction} challenge, but since no benchmark evaluating OOD detection and adaption currently exists to assess \emph{control}, we introduce an autonomous car novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of driving agents to a suite of tasks with distribution shifts.},
  archiveprefix = {arXiv},
  eprint        = {2006.14911},
  file          = {:http\://arxiv.org/pdf/2006.14911v2:PDF},
  keywords      = {cs.LG, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Dosovitskiy2017,
  author        = {Alexey Dosovitskiy and German Ros and Felipe Codevilla and Antonio Lopez and Vladlen Koltun},
  title         = {CARLA: An Open Urban Driving Simulator},
  year          = {2017},
  month         = nov,
  abstract      = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E},
  archiveprefix = {arXiv},
  eprint        = {1711.03938},
  file          = {:http\://arxiv.org/pdf/1711.03938v1:PDF},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO},
  primaryclass  = {cs.LG},
}

@Article{Ballas2015,
  author        = {Nicolas Ballas and Li Yao and Chris Pal and Aaron Courville},
  title         = {Delving Deeper into Convolutional Networks for Learning Video Representations},
  year          = {2015},
  month         = nov,
  abstract      = {We propose an approach to learn spatio-temporal features in videos from intermediate visual representations we call "percepts" using Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts that are extracted from all level of a deep convolutional network trained on the large ImageNet dataset. While high-level percepts contain highly discriminative information, they tend to have a low-spatial resolution. Low-level percepts, on the other hand, preserve a higher spatial resolution from which we can model finer motion patterns. Using low-level percepts can leads to high-dimensionality video representations. To mitigate this effect and control the model number of parameters, we introduce a variant of the GRU model that leverages the convolution operations to enforce sparse connectivity of the model units and share parameters across the input spatial locations. We empirically validate our approach on both Human Action Recognition and Video Captioning tasks. In particular, we achieve results equivalent to state-of-art on the YouTube2Text dataset using a simpler text-decoder model and without extra 3D CNN features.},
  archiveprefix = {arXiv},
  eprint        = {1511.06432},
  file          = {:http\://arxiv.org/pdf/1511.06432v4:PDF},
  keywords      = {cs.CV, cs.LG, cs.NE},
  primaryclass  = {cs.CV},
}

@Article{Chung2014,
  author        = {Junyoung Chung and Caglar Gulcehre and KyungHyun Cho and Yoshua Bengio},
  title         = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  year          = {2014},
  month         = dec,
  abstract      = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  archiveprefix = {arXiv},
  eprint        = {1412.3555},
  file          = {:http\://arxiv.org/pdf/1412.3555v1:PDF},
  keywords      = {cs.NE, cs.LG},
  primaryclass  = {cs.NE},
}

@Article{Geiger2012AreWR,
  author  = {Andreas Geiger and Philip Lenz and Raquel Urtasun},
  journal = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  title   = {Are we ready for autonomous driving? The KITTI vision benchmark suite},
  year    = {2012},
  pages   = {3354-3361},
}

@InProceedings{Argoverse2,
  author    = {Benjamin Wilson and William Qi and Tanmay Agarwal and John Lambert and Jagjeet Singh and Siddhesh Khandelwal and Bowen Pan and Ratnesh Kumar and Andrew Hartnett and Jhony Kaesemodel Pontes and Deva Ramanan and Peter Carr and James Hays},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)},
  title     = {Argoverse 2: Next Generation Datasets for Self-driving Perception and Forecasting},
  year      = {2021},
}

@InProceedings{Muller2006,
  author    = {Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat and Cun, Yann L},
  booktitle = {NeurIPS},
  title     = {Off-road obstacle avoidance through end-to-end learning},
  year      = {2006},
}

@InProceedings{Wang2019a,
  author    = {Wang, Dequan and Devin, Coline and Cai, Qi-Zhi and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  booktitle = {IROS},
  title     = {Monocular plan view networks for autonomous driving},
  year      = {2019},
}

@Article{Caron2020,
  author        = {Mathilde Caron and Ishan Misra and Julien Mairal and Priya Goyal and Piotr Bojanowski and Armand Joulin},
  title         = {Unsupervised Learning of Visual Features by Contrasting Cluster Assignments},
  year          = {2020},
  month         = jun,
  abstract      = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archiveprefix = {arXiv},
  eprint        = {2006.09882},
  file          = {:http\://arxiv.org/pdf/2006.09882v5:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Laskin2020,
  author        = {Michael Laskin and Kimin Lee and Adam Stooke and Lerrel Pinto and Pieter Abbeel and Aravind Srinivas},
  title         = {Reinforcement Learning with Augmented Data},
  year          = {2020},
  month         = apr,
  abstract      = {Learning from visual observations is a fundamental yet challenging problem in Reinforcement Learning (RL). Although algorithmic advances combined with convolutional neural networks have proved to be a recipe for success, current methods are still lacking on two fronts: (a) data-efficiency of learning and (b) generalization to new environments. To this end, we present Reinforcement Learning with Augmented Data (RAD), a simple plug-and-play module that can enhance most RL algorithms. We perform the first extensive study of general data augmentations for RL on both pixel-based and state-based inputs, and introduce two new data augmentations - random translate and random amplitude scale. We show that augmentations such as random translate, crop, color jitter, patch cutout, random convolutions, and amplitude scale can enable simple RL algorithms to outperform complex state-of-the-art methods across common benchmarks. RAD sets a new state-of-the-art in terms of data-efficiency and final performance on the DeepMind Control Suite benchmark for pixel-based control as well as OpenAI Gym benchmark for state-based control. We further demonstrate that RAD significantly improves test-time generalization over existing methods on several OpenAI ProcGen benchmarks. Our RAD module and training code are available at https://www.github.com/MishaLaskin/rad.},
  archiveprefix = {arXiv},
  eprint        = {2004.14990},
  file          = {:http\://arxiv.org/pdf/2004.14990v5:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Kostrikov2020,
  author        = {Ilya Kostrikov and Denis Yarats and Rob Fergus},
  title         = {Image Augmentation Is All You Need: Regularizing Deep Reinforcement Learning from Pixels},
  year          = {2020},
  month         = apr,
  abstract      = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
  archiveprefix = {arXiv},
  eprint        = {2004.13649},
  file          = {:http\://arxiv.org/pdf/2004.13649v4:PDF},
  keywords      = {cs.LG, cs.CV, eess.IV, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Yarats2021,
  author        = {Denis Yarats and Rob Fergus and Alessandro Lazaric and Lerrel Pinto},
  title         = {Mastering Visual Continuous Control: Improved Data-Augmented Reinforcement Learning},
  year          = {2021},
  month         = jul,
  abstract      = {We present DrQ-v2, a model-free reinforcement learning (RL) algorithm for visual continuous control. DrQ-v2 builds on DrQ, an off-policy actor-critic approach that uses data augmentation to learn directly from pixels. We introduce several improvements that yield state-of-the-art results on the DeepMind Control Suite. Notably, DrQ-v2 is able to solve complex humanoid locomotion tasks directly from pixel observations, previously unattained by model-free RL. DrQ-v2 is conceptually simple, easy to implement, and provides significantly better computational footprint compared to prior work, with the majority of tasks taking just 8 hours to train on a single GPU. Finally, we publicly release DrQ-v2's implementation to provide RL practitioners with a strong and computationally efficient baseline.},
  archiveprefix = {arXiv},
  eprint        = {2107.09645},
  file          = {:http\://arxiv.org/pdf/2107.09645v1:PDF},
  keywords      = {cs.AI, cs.LG},
  primaryclass  = {cs.AI},
}

@Article{Mezghani2021,
  author        = {Lina Mezghani and Sainbayar Sukhbaatar and Thibaut Lavril and Oleksandr Maksymets and Dhruv Batra and Piotr Bojanowski and Karteek Alahari},
  journal       = {IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022},
  title         = {Memory-Augmented Reinforcement Learning for Image-Goal Navigation},
  year          = {2021},
  month         = jan,
  abstract      = {In this work, we present a memory-augmented approach for image-goal navigation. Earlier attempts, including RL-based and SLAM-based approaches have either shown poor generalization performance, or are heavily-reliant on pose/depth sensors. Our method is based on an attention-based end-to-end model that leverages an episodic memory to learn to navigate. First, we train a state-embedding network in a self-supervised fashion, and then use it to embed previously-visited states into the agent's memory. Our navigation policy takes advantage of this information through an attention mechanism. We validate our approach with extensive evaluations, and show that our model establishes a new state of the art on the challenging Gibson dataset. Furthermore, we achieve this impressive performance from RGB input alone, without access to additional information such as position or depth, in stark contrast to related work.},
  archiveprefix = {arXiv},
  eprint        = {2101.05181},
  file          = {:http\://arxiv.org/pdf/2101.05181v5:PDF},
  keywords      = {cs.CV, cs.RO},
  primaryclass  = {cs.CV},
}

@Book{Janai2020,
  author       = {Joel Janai and Fatma Güney and Aseem Behl and Andreas Geiger},
  publisher    = {Foundations and Trends in Computer Graphics and Vision},
  title        = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State of the Art},
  year         = {2020},
  number       = {1-3},
  volume       = {12},
  abstract     = {Recent years have witnessed enormous progress in AI-related fields such as computer vision, machine learning, and autonomous vehicles. As with any rapidly growing field, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several survey papers on particular sub-problems have appeared, no comprehensive survey on problems, datasets, and methods in computer vision for autonomous vehicles has been published. This monograph attempts to narrow this gap by providing a survey on the state-of-the-art datasets and techniques. Our survey includes both the historically most relevant literature as well as the current state of the art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding, and end-to-end learning for autonomous driving. Towards this goal, we analyze the performance of the state of the art on several challenging benchmarking datasets, including KITTI, MOT, and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we also provide a website that allows navigating topics as well as methods and provides additional information.},
  cvlibs_arxiv = {https://arxiv.org/abs/1704.05519},
  cvlibs_link  = {http://dx.doi.org/10.1561/0600000079},
  cvlibs_month = {07},
  groups       = {cvlibs},
}

@Article{Tampuu2020,
  author        = {Ardi Tampuu and Maksym Semikin and Naveed Muhammad and Dmytro Fishman and Tambet Matiisen},
  journal       = {IEEE Transactions on Neural Networks and Learning Systems, 2020},
  title         = {A Survey of End-to-End Driving: Architectures and Training Methods},
  year          = {2020},
  month         = mar,
  abstract      = {Autonomous driving is of great interest to industry and academia alike. The use of machine learning approaches for autonomous driving has long been studied, but mostly in the context of perception. In this paper we take a deeper look on the so called end-to-end approaches for autonomous driving, where the entire driving pipeline is replaced with a single neural network. We review the learning methods, input and output modalities, network architectures and evaluation schemes in end-to-end driving literature. Interpretability and safety are discussed separately, as they remain challenging for this approach. Beyond providing a comprehensive overview of existing methods, we conclude the review with an architecture that combines the most promising elements of the end-to-end autonomous driving systems.},
  archiveprefix = {arXiv},
  doi           = {10.1109/TNNLS.2020.3043505},
  eprint        = {2003.06404},
  file          = {:http\://arxiv.org/pdf/2003.06404v2:PDF},
  keywords      = {cs.AI, cs.RO, 68T40, I.2.9},
  primaryclass  = {cs.AI},
}

@Article{Bojarski2016,
  author        = {Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
  title         = {End to End Learning for Self-Driving Cars},
  year          = {2016},
  month         = apr,
  abstract      = {We trained a convolutional neural network (CNN) to map raw pixels from a single front-facing camera directly to steering commands. This end-to-end approach proved surprisingly powerful. With minimum training data from humans the system learns to drive in traffic on local roads with or without lane markings and on highways. It also operates in areas with unclear visual guidance such as in parking lots and on unpaved roads. The system automatically learns internal representations of the necessary processing steps such as detecting useful road features with only the human steering angle as the training signal. We never explicitly trained it to detect, for example, the outline of roads. Compared to explicit decomposition of the problem, such as lane marking detection, path planning, and control, our end-to-end system optimizes all processing steps simultaneously. We argue that this will eventually lead to better performance and smaller systems. Better performance will result because the internal components self-optimize to maximize overall system performance, instead of optimizing human-selected intermediate criteria, e.g., lane detection. Such criteria understandably are selected for ease of human interpretation which doesn't automatically guarantee maximum system performance. Smaller networks are possible because the system learns to solve the problem with the minimal number of processing steps. We used an NVIDIA DevBox and Torch 7 for training and an NVIDIA DRIVE(TM) PX self-driving car computer also running Torch 7 for determining where to drive. The system operates at 30 frames per second (FPS).},
  archiveprefix = {arXiv},
  eprint        = {1604.07316},
  file          = {:http\://arxiv.org/pdf/1604.07316v1:PDF},
  keywords      = {cs.CV, cs.LG, cs.NE},
  primaryclass  = {cs.CV},
}

@InProceedings{Pomerleau1988,
  author    = {Pomerleau, Dean A.},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {ALVINN: An Autonomous Land Vehicle in a Neural Network},
  year      = {1988},
  editor    = {D. Touretzky},
  publisher = {Morgan-Kaufmann},
  volume    = {1},
  url       = {https://proceedings.neurips.cc/paper/1988/file/812b4ba287f5ee0bc9d43bbf5bbe87fb-Paper.pdf},
}

@Article{Casas2021,
  author        = {Sergio Casas and Abbas Sadat and Raquel Urtasun},
  title         = {MP3: A Unified Model to Map, Perceive, Predict and Plan},
  year          = {2021},
  month         = jan,
  abstract      = {High-definition maps (HD maps) are a key component of most modern self-driving systems due to their valuable semantic and geometric information. Unfortunately, building HD maps has proven hard to scale due to their cost as well as the requirements they impose in the localization system that has to work everywhere with centimeter-level accuracy. Being able to drive without an HD map would be very beneficial to scale self-driving solutions as well as to increase the failure tolerance of existing ones (e.g., if localization fails or the map is not up-to-date). Towards this goal, we propose MP3, an end-to-end approach to mapless driving where the input is raw sensor data and a high-level command (e.g., turn left at the intersection). MP3 predicts intermediate representations in the form of an online map and the current and future state of dynamic agents, and exploits them in a novel neural motion planner to make interpretable decisions taking into account uncertainty. We show that our approach is significantly safer, more comfortable, and can follow commands better than the baselines in challenging long-term closed-loop simulations, as well as when compared to an expert driver in a large-scale real-world dataset.},
  archiveprefix = {arXiv},
  eprint        = {2101.06806},
  file          = {:http\://arxiv.org/pdf/2101.06806v1:PDF},
  keywords      = {cs.RO, cs.AI, cs.CV, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{Xiao2019,
  author        = {Yi Xiao and Felipe Codevilla and Akhil Gurram and Onay Urfalioglu and Antonio M. López},
  title         = {Multimodal End-to-End Autonomous Driving},
  year          = {2019},
  month         = jun,
  abstract      = {A crucial component of an autonomous vehicle (AV) is the artificial intelligence (AI) is able to drive towards a desired destination. Today, there are different paradigms addressing the development of AI drivers. On the one hand, we find modular pipelines, which divide the driving task into sub-tasks such as perception and maneuver planning and control. On the other hand, we find end-to-end driving approaches that try to learn a direct mapping from input raw sensor data to vehicle control signals. The later are relatively less studied, but are gaining popularity since they are less demanding in terms of sensor data annotation. This paper focuses on end-to-end autonomous driving. So far, most proposals relying on this paradigm assume RGB images as input sensor data. However, AVs will not be equipped only with cameras, but also with active sensors providing accurate depth information (e.g., LiDARs). Accordingly, this paper analyses whether combining RGB and depth modalities, i.e. using RGBD data, produces better end-to-end AI drivers than relying on a single modality. We consider multimodality based on early, mid and late fusion schemes, both in multisensory and single-sensor (monocular depth estimation) settings. Using the CARLA simulator and conditional imitation learning (CIL), we show how, indeed, early fusion multimodality outperforms single-modality.},
  archiveprefix = {arXiv},
  doi           = {10.1109/TITS.2020.3013234},
  eprint        = {1906.03199},
  file          = {:http\://arxiv.org/pdf/1906.03199v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@InProceedings{Prakash2020,
  author    = {Prakash, Aditya and Behl, Aseem and Ohn-Bar, Eshed and Chitta, Kashyap and Geiger, Andreas},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  title     = {Exploring Data Aggregation in Policy Learning for Vision-Based Urban Autonomous Driving},
  year      = {2020},
  month     = {June},
}

@Article{Sauer2018,
  author        = {Axel Sauer and Nikolay Savinov and Andreas Geiger},
  title         = {Conditional Affordance Learning for Driving in Urban Environments},
  year          = {2018},
  month         = jun,
  abstract      = {Most existing approaches to autonomous driving fall into one of two categories: modular pipelines, that build an extensive model of the environment, and imitation learning approaches, that map images directly to control outputs. A recently proposed third paradigm, direct perception, aims to combine the advantages of both by using a neural network to learn appropriate low-dimensional intermediate representations. However, existing direct perception approaches are restricted to simple highway situations, lacking the ability to navigate intersections, stop at traffic lights or respect speed limits. In this work, we propose a direct perception approach which maps video input to intermediate representations suitable for autonomous navigation in complex urban environments given high-level directional inputs. Compared to state-of-the-art reinforcement and conditional imitation learning approaches, we achieve an improvement of up to 68 % in goal-directed navigation on the challenging CARLA simulation benchmark. In addition, our approach is the first to handle traffic lights and speed signs by using image-level labels only, as well as smooth car-following, resulting in a significant reduction of traffic accidents in simulation.},
  archiveprefix = {arXiv},
  eprint        = {1806.06498},
  file          = {:http\://arxiv.org/pdf/1806.06498v3:PDF},
  keywords      = {cs.RO, cs.LG, cs.SY},
  primaryclass  = {cs.RO},
}

@Article{Xiao2020,
  author        = {Yi Xiao and Felipe Codevilla and Christopher Pal and Antonio M. Lopez},
  title         = {Action-Based Representation Learning for Autonomous Driving},
  year          = {2020},
  month         = aug,
  abstract      = {Human drivers produce a vast amount of data which could, in principle, be used to improve autonomous driving systems. Unfortunately, seemingly straightforward approaches for creating end-to-end driving models that map sensor data directly into driving actions are problematic in terms of interpretability, and typically have significant difficulty dealing with spurious correlations. Alternatively, we propose to use this kind of action-based driving data for learning representations. Our experiments show that an affordance-based driving model pre-trained with this approach can leverage a relatively small amount of weakly annotated imagery and outperform pure end-to-end driving models, while being more interpretable. Further, we demonstrate how this strategy outperforms previous methods based on learning inverse dynamics models as well as other methods based on heavy human supervision (ImageNet).},
  archiveprefix = {arXiv},
  eprint        = {2008.09417},
  file          = {:http\://arxiv.org/pdf/2008.09417v2:PDF},
  keywords      = {cs.CV, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@InProceedings{chen2021learning,
  author    = {Chen, Dian and Koltun, Vladlen and Kr{\"a}henb{\"u}hl, Philipp},
  booktitle = {ICCV},
  title     = {Learning to drive from a world on rails},
  year      = {2021},
}

@Article{Toromanoff2019,
  author        = {Marin Toromanoff and Emilie Wirbel and Fabien Moutarde},
  title         = {End-to-End Model-Free Reinforcement Learning for Urban Driving using Implicit Affordances},
  year          = {2019},
  month         = nov,
  abstract      = {Reinforcement Learning (RL) aims at learning an optimal behavior policy from its own experiments and not rule-based control methods. However, there is no RL algorithm yet capable of handling a task as difficult as urban driving. We present a novel technique, coined implicit affordances, to effectively leverage RL for urban driving thus including lane keeping, pedestrians and vehicles avoidance, and traffic light detection. To our knowledge we are the first to present a successful RL agent handling such a complex task especially regarding the traffic light detection. Furthermore, we have demonstrated the effectiveness of our method by winning the Camera Only track of the CARLA challenge.},
  archiveprefix = {arXiv},
  eprint        = {1911.10868},
  file          = {:http\://arxiv.org/pdf/1911.10868v2:PDF},
  keywords      = {cs.LG, cs.AI, cs.CV, cs.RO, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Wang2020a,
  author        = {Jingke Wang and Yue Wang and Dongkun Zhang and Yezhou Yang and Rong Xiong},
  title         = {Learning hierarchical behavior and motion planning for autonomous driving},
  year          = {2020},
  month         = may,
  abstract      = {Learning-based driving solution, a new branch for autonomous driving, is expected to simplify the modeling of driving by learning the underlying mechanisms from data. To improve the tactical decision-making for learning-based driving solution, we introduce hierarchical behavior and motion planning (HBMP) to explicitly model the behavior in learning-based solution. Due to the coupled action space of behavior and motion, it is challenging to solve HBMP problem using reinforcement learning (RL) for long-horizon driving tasks. We transform HBMP problem by integrating a classical sampling-based motion planner, of which the optimal cost is regarded as the rewards for high-level behavior learning. As a result, this formulation reduces action space and diversifies the rewards without losing the optimality of HBMP. In addition, we propose a sharable representation for input sensory data across simulation platforms and real-world environment, so that models trained in a fast event-based simulator, SUMO, can be used to initialize and accelerate the RL training in a dynamics based simulator, CARLA. Experimental results demonstrate the effectiveness of the method. Besides, the model is successfully transferred to the real-world, validating the generalization capability.},
  archiveprefix = {arXiv},
  eprint        = {2005.03863},
  file          = {:http\://arxiv.org/pdf/2005.03863v1:PDF},
  keywords      = {cs.RO, cs.AI},
  primaryclass  = {cs.RO},
}

@Article{article,
  author = {Bain, Michael and Sammut, Claude},
  title  = {A Framework for Behavioural Cloning},
  year   = {2000},
  month  = {03},
  volume = {15},
}

@InProceedings{7410669,
  author    = {Chen, Chenyi and Seff, Ari and Kornhauser, Alain and Xiao, Jianxiong},
  booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
  title     = {DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving},
  year      = {2015},
  pages     = {2722-2730},
  doi       = {10.1109/ICCV.2015.312},
}

@Article{Gupta2017,
  author        = {Saurabh Gupta and Varun Tolani and James Davidson and Sergey Levine and Rahul Sukthankar and Jitendra Malik},
  title         = {Cognitive Mapping and Planning for Visual Navigation},
  year          = {2017},
  month         = feb,
  abstract      = {We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as 'going to a chair'. We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.},
  archiveprefix = {arXiv},
  eprint        = {1702.03920},
  file          = {:http\://arxiv.org/pdf/1702.03920v3:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Li2018,
  author        = {Guohao Li and Matthias Müller and Vincent Casser and Neil Smith and Dominik L. Michels and Bernard Ghanem},
  title         = {OIL: Observational Imitation Learning},
  year          = {2018},
  month         = mar,
  abstract      = {Recent work has explored the problem of autonomous navigation by imitating a teacher and learning an end-to-end policy, which directly predicts controls from raw images. However, these approaches tend to be sensitive to mistakes by the teacher and do not scale well to other environments or vehicles. To this end, we propose Observational Imitation Learning (OIL), a novel imitation learning variant that supports online training and automatic selection of optimal behavior by observing multiple imperfect teachers. We apply our proposed methodology to the challenging problems of autonomous driving and UAV racing. For both tasks, we utilize the Sim4CV simulator that enables the generation of large amounts of synthetic training data and also allows for online learning and evaluation. We train a perception network to predict waypoints from raw image data and use OIL to train another network to predict controls from these waypoints. Extensive experiments demonstrate that our trained network outperforms its teachers, conventional imitation learning (IL) and reinforcement learning (RL) baselines and even humans in simulation. The project website is available at https://sites.google.com/kaust.edu.sa/oil/ and a video at https://youtu.be/_rhq8a0qgeg},
  archiveprefix = {arXiv},
  eprint        = {1803.01129},
  file          = {:http\://arxiv.org/pdf/1803.01129v3:PDF},
  keywords      = {cs.CV, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@Article{Liang2018,
  author        = {Xiaodan Liang and Tairui Wang and Luona Yang and Eric Xing},
  title         = {CIRL: Controllable Imitative Reinforcement Learning for Vision-based Self-driving},
  year          = {2018},
  month         = jul,
  abstract      = {Autonomous urban driving navigation with complex multi-agent dynamics is under-explored due to the difficulty of learning an optimal driving policy. The traditional modular pipeline heavily relies on hand-designed rules and the pre-processing perception system while the supervised learning-based models are limited by the accessibility of extensive human experience. We present a general and principled Controllable Imitative Reinforcement Learning (CIRL) approach which successfully makes the driving agent achieve higher success rates based on only vision inputs in a high-fidelity car simulator. To alleviate the low exploration efficiency for large continuous action space that often prohibits the use of classical RL on challenging real tasks, our CIRL explores over a reasonably constrained action space guided by encoded experiences that imitate human demonstrations, building upon Deep Deterministic Policy Gradient (DDPG). Moreover, we propose to specialize adaptive policies and steering-angle reward designs for different control signals (i.e. follow, straight, turn right, turn left) based on the shared representations to improve the model capability in tackling with diverse cases. Extensive experiments on CARLA driving benchmark demonstrate that CIRL substantially outperforms all previous methods in terms of the percentage of successfully completed episodes on a variety of goal-directed driving tasks. We also show its superior generalization capability in unseen environments. To our knowledge, this is the first successful case of the learned driving policy through reinforcement learning in the high-fidelity simulator, which performs better-than supervised imitation learning.},
  archiveprefix = {arXiv},
  eprint        = {1807.03776},
  file          = {:http\://arxiv.org/pdf/1807.03776v1:PDF},
  keywords      = {cs.CV, cs.RO},
  primaryclass  = {cs.CV},
}

@InProceedings{inproceedings,
  author = {Lecun, Yann and Muller, Urs and Ben, Jan and Cosatto, Eric and Flepp, Beat},
  title  = {Off-Road Obstacle Avoidance through End-to-End Learning.},
  year   = {2005},
  month  = {01},
}

@Article{Osa2018,
  author        = {Takayuki Osa and Joni Pajarinen and Gerhard Neumann and J. Andrew Bagnell and Pieter Abbeel and Jan Peters},
  title         = {An Algorithmic Perspective on Imitation Learning},
  year          = {2018},
  month         = nov,
  abstract      = {As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning.},
  archiveprefix = {arXiv},
  doi           = {10.1561/2300000053},
  eprint        = {1811.06711},
  file          = {:http\://arxiv.org/pdf/1811.06711v1:PDF},
  keywords      = {cs.RO, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{DBLP:journals/corr/abs-1912-02973,
  author   = {Albert Zhao and Tong He and Yitao Liang and Haibin Huang and Guy Van den Broeck and Stefano Soatto},
  journal  = {CoRR},
  title    = {LaTeS: Latent Space Distillation for Teacher-Student Driving Policy Learning},
  year     = {2019},
  volume   = {abs/1912.02973},
  cdate    = {1546300800000},
  publtype = {informal},
  url      = {http://arxiv.org/abs/1912.02973},
}

@Article{Zhou2019a,
  author        = {Brady Zhou and Philipp Krähenbühl and Vladlen Koltun},
  journal       = {Science Robotics 22 May 2019: Vol. 4, Issue 30, eaaw6661},
  title         = {Does computer vision matter for action?},
  year          = {2019},
  month         = may,
  abstract      = {Computer vision produces representations of scene content. Much computer vision research is predicated on the assumption that these intermediate representations are useful for action. Recent work at the intersection of machine learning and robotics calls this assumption into question by training sensorimotor systems directly for the task at hand, from pixels to actions, with no explicit intermediate representations. Thus the central question of our work: Does computer vision matter for action? We probe this question and its offshoots via immersive simulation, which allows us to conduct controlled reproducible experiments at scale. We instrument immersive three-dimensional environments to simulate challenges such as urban driving, off-road trail traversal, and battle. Our main finding is that computer vision does matter. Models equipped with intermediate representations train faster, achieve higher task performance, and generalize better to previously unseen environments. A video that summarizes the work and illustrates the results can be found at https://youtu.be/4MfWa2yZ0Jc},
  archiveprefix = {arXiv},
  doi           = {10.1126/scirobotics.aaw6661},
  eprint        = {1905.12887},
  file          = {:http\://arxiv.org/pdf/1905.12887v2:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG, cs.RO},
  primaryclass  = {cs.CV},
}

@InProceedings{Chang2020,
  author    = {Chang, Matthew and Gupta, Arjun and Gupta, Saurabh},
  booktitle = {Advances in Neural Information Processing Systems},
  title     = {Semantic Visual Navigation by Watching YouTube Videos},
  year      = {2020},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
  pages     = {4283--4294},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  file      = {:2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf:PDF},
  ranking   = {rank5},
  url       = {https://proceedings.neurips.cc/paper/2020/file/2cd4e8a2ce081c3d7c32c3cde4312ef7-Paper.pdf},
}

@Article{Torabi2018,
  author        = {Faraz Torabi and Garrett Warnell and Peter Stone},
  title         = {Behavioral Cloning from Observation},
  year          = {2018},
  month         = may,
  abstract      = {Humans often learn how to perform tasks via imitation: they observe others perform a task, and then very quickly infer the appropriate actions to take based on their observations. While extending this paradigm to autonomous agents is a well-studied problem in general, there are two particular aspects that have largely been overlooked: (1) that the learning is done from observation only (i.e., without explicit action information), and (2) that the learning is typically done very quickly. In this work, we propose a two-phase, autonomous imitation learning technique called behavioral cloning from observation (BCO), that aims to provide improved performance with respect to both of these aspects. First, we allow the agent to acquire experience in a self-supervised fashion. This experience is used to develop a model which is then utilized to learn a particular task by observing an expert perform that task without the knowledge of the specific actions taken. We experimentally compare BCO to imitation learning methods, including the state-of-the-art, generative adversarial imitation learning (GAIL) technique, and we show comparable task performance in several different simulation domains while exhibiting increased learning speed after expert trajectories become available.},
  archiveprefix = {arXiv},
  eprint        = {1805.01954},
  file          = {:http\://arxiv.org/pdf/1805.01954v2:PDF},
  keywords      = {cs.AI},
  primaryclass  = {cs.AI},
  ranking       = {rank5},
}

@Article{Torabi2019,
  author        = {Faraz Torabi and Garrett Warnell and Peter Stone},
  title         = {Recent Advances in Imitation Learning from Observation},
  year          = {2019},
  month         = may,
  abstract      = {Imitation learning is the process by which one agent tries to learn how to perform a certain task using information generated by another, often more-expert agent performing that same task. Conventionally, the imitator has access to both state and action information generated by an expert performing the task (e.g., the expert may provide a kinesthetic demonstration of object placement using a robotic arm). However, requiring the action information prevents imitation learning from a large number of existing valuable learning resources such as online videos of humans performing tasks. To overcome this issue, the specific problem of imitation from observation (IfO) has recently garnered a great deal of attention, in which the imitator only has access to the state information (e.g., video frames) generated by the expert. In this paper, we provide a literature review of methods developed for IfO, and then point out some open research problems and potential future work.},
  archiveprefix = {arXiv},
  eprint        = {1905.13566},
  file          = {:http\://arxiv.org/pdf/1905.13566v2:PDF},
  keywords      = {cs.RO, cs.AI, cs.LG},
  primaryclass  = {cs.RO},
}

@Article{Sun2019,
  author        = {Wen Sun and Anirudh Vemula and Byron Boots and J. Andrew Bagnell},
  title         = {Provably Efficient Imitation Learning from Observation Alone},
  year          = {2019},
  month         = may,
  abstract      = {We study Imitation Learning (IL) from Observations alone (ILFO) in large-scale MDPs. While most IL algorithms rely on an expert to directly provide actions to the learner, in this setting the expert only supplies sequences of observations. We design a new model-free algorithm for ILFO, Forward Adversarial Imitation Learning (FAIL), which learns a sequence of time-dependent policies by minimizing an Integral Probability Metric between the observation distributions of the expert policy and the learner. FAIL is the first provably efficient algorithm in ILFO setting, which learns a near-optimal policy with a number of samples that is polynomial in all relevant parameters but independent of the number of unique observations. The resulting theory extends the domain of provably sample efficient learning algorithms beyond existing results, which typically only consider tabular reinforcement learning settings or settings that require access to a near-optimal reset distribution. We also investigate the extension of FAIL in a model-based setting. Finally we demonstrate the efficacy of FAIL on multiple OpenAI Gym control tasks.},
  archiveprefix = {arXiv},
  eprint        = {1905.10948},
  file          = {:http\://arxiv.org/pdf/1905.10948v2:PDF},
  keywords      = {cs.LG, stat.ML},
  primaryclass  = {cs.LG},
}

@Article{Lee2013,
  author  = {Lee, Dong-Hyun},
  journal = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)},
  title   = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
  year    = {2013},
  month   = {07},
}

@Article{Lin2021,
  author        = {Zhiqiu Lin and Deva Ramanan and Aayush Bansal},
  title         = {Streaming Self-Training via Domain-Agnostic Unlabeled Images},
  year          = {2021},
  month         = apr,
  abstract      = {We present streaming self-training (SST) that aims to democratize the process of learning visual recognition models such that a non-expert user can define a new task depending on their needs via a few labeled examples and minimal domain knowledge. Key to SST are two crucial observations: (1) domain-agnostic unlabeled images enable us to learn better models with a few labeled examples without any additional knowledge or supervision; and (2) learning is a continuous process and can be done by constructing a schedule of learning updates that iterates between pre-training on novel segments of the streams of unlabeled data, and fine-tuning on the small and fixed labeled dataset. This allows SST to overcome the need for a large number of domain-specific labeled and unlabeled examples, exorbitant computational resources, and domain/task-specific knowledge. In this setting, classical semi-supervised approaches require a large amount of domain-specific labeled and unlabeled examples, immense resources to process data, and expert knowledge of a particular task. Due to these reasons, semi-supervised learning has been restricted to a few places that can house required computational and human resources. In this work, we overcome these challenges and demonstrate our findings for a wide range of visual recognition tasks including fine-grained image classification, surface normal estimation, and semantic segmentation. We also demonstrate our findings for diverse domains including medical, satellite, and agricultural imagery, where there does not exist a large amount of labeled or unlabeled data.},
  archiveprefix = {arXiv},
  eprint        = {2104.03309},
  file          = {:http\://arxiv.org/pdf/2104.03309v1:PDF},
  keywords      = {cs.CV, cs.AI, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Rizve2021,
  author        = {Mamshad Nayeem Rizve and Kevin Duarte and Yogesh S Rawat and Mubarak Shah},
  title         = {In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning},
  year          = {2021},
  month         = jan,
  abstract      = {The recent research in semi-supervised learning (SSL) is mostly dominated by consistency regularization based methods which achieve strong performance. However, they heavily rely on domain-specific data augmentations, which are not easy to generate for all data modalities. Pseudo-labeling (PL) is a general SSL approach that does not have this constraint but performs relatively poorly in its original formulation. We argue that PL underperforms due to the erroneous high confidence predictions from poorly calibrated models; these predictions generate many incorrect pseudo-labels, leading to noisy training. We propose an uncertainty-aware pseudo-label selection (UPS) framework which improves pseudo labeling accuracy by drastically reducing the amount of noise encountered in the training process. Furthermore, UPS generalizes the pseudo-labeling process, allowing for the creation of negative pseudo-labels; these negative pseudo-labels can be used for multi-label classification as well as negative learning to improve the single-label classification. We achieve strong performance when compared to recent SSL methods on the CIFAR-10 and CIFAR-100 datasets. Also, we demonstrate the versatility of our method on the video dataset UCF-101 and the multi-label dataset Pascal VOC.},
  archiveprefix = {arXiv},
  eprint        = {2101.06329},
  file          = {:http\://arxiv.org/pdf/2101.06329v3:PDF},
  keywords      = {cs.LG, cs.CV},
  primaryclass  = {cs.LG},
}

@Article{Saito2017,
  author        = {Kuniaki Saito and Yoshitaka Ushiku and Tatsuya Harada},
  title         = {Asymmetric Tri-training for Unsupervised Domain Adaptation},
  year          = {2017},
  month         = feb,
  abstract      = {Deep-layered models trained on a large number of labeled samples boost the accuracy of many tasks. It is important to apply such models to different domains because collecting many labeled samples in various domains is expensive. In unsupervised domain adaptation, one needs to train a classifier that works well on a target domain when provided with labeled source samples and unlabeled target samples. Although many methods aim to match the distributions of source and target samples, simply matching the distribution cannot ensure accuracy on the target domain. To learn discriminative representations for the target domain, we assume that artificially labeling target samples can result in a good representation. Tri-training leverages three classifiers equally to give pseudo-labels to unlabeled samples, but the method does not assume labeling samples generated from a different domain.In this paper, we propose an asymmetric tri-training method for unsupervised domain adaptation, where we assign pseudo-labels to unlabeled samples and train neural networks as if they are true labels. In our work, we use three networks asymmetrically. By asymmetric, we mean that two networks are used to label unlabeled target samples and one network is trained by the samples to obtain target-discriminative representations. We evaluate our method on digit recognition and sentiment analysis datasets. Our proposed method achieves state-of-the-art performance on the benchmark digit recognition datasets of domain adaptation.},
  archiveprefix = {arXiv},
  eprint        = {1702.08400},
  file          = {:http\://arxiv.org/pdf/1702.08400v3:PDF},
  keywords      = {cs.CV, cs.AI},
  primaryclass  = {cs.CV},
}

@Article{Caine2021,
  author        = {Benjamin Caine and Rebecca Roelofs and Vijay Vasudevan and Jiquan Ngiam and Yuning Chai and Zhifeng Chen and Jonathon Shlens},
  title         = {Pseudo-labeling for Scalable 3D Object Detection},
  year          = {2021},
  month         = mar,
  abstract      = {To safely deploy autonomous vehicles, onboard perception systems must work reliably at high accuracy across a diverse set of environments and geographies. One of the most common techniques to improve the efficacy of such systems in new domains involves collecting large labeled datasets, but such datasets can be extremely costly to obtain, especially if each new deployment geography requires additional data with expensive 3D bounding box annotations. We demonstrate that pseudo-labeling for 3D object detection is an effective way to exploit less expensive and more widely available unlabeled data, and can lead to performance gains across various architectures, data augmentation strategies, and sizes of the labeled dataset. Overall, we show that better teacher models lead to better student models, and that we can distill expensive teachers into efficient, simple students. Specifically, we demonstrate that pseudo-label-trained student models can outperform supervised models trained on 3-10 times the amount of labeled examples. Using PointPillars [24], a two-year-old architecture, as our student model, we are able to achieve state of the art accuracy simply by leveraging large quantities of pseudo-labeled data. Lastly, we show that these student models generalize better than supervised models to a new domain in which we only have unlabeled data, making pseudo-label training an effective form of unsupervised domain adaptation.},
  archiveprefix = {arXiv},
  eprint        = {2103.02093},
  file          = {:http\://arxiv.org/pdf/2103.02093v1:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Yang2021,
  author        = {Jihan Yang and Shaoshuai Shi and Zhe Wang and Hongsheng Li and Xiaojuan Qi},
  title         = {ST3D: Self-training for Unsupervised Domain Adaptation on 3D Object Detection},
  year          = {2021},
  month         = mar,
  abstract      = {We present a new domain adaptive self-training pipeline, named ST3D, for unsupervised domain adaptation on 3D object detection from point clouds. First, we pre-train the 3D detector on the source domain with our proposed random object scaling strategy for mitigating the negative effects of source domain bias. Then, the detector is iteratively improved on the target domain by alternatively conducting two steps, which are the pseudo label updating with the developed quality-aware triplet memory bank and the model training with curriculum data augmentation. These specific designs for 3D object detection enable the detector to be trained with consistent and high-quality pseudo labels and to avoid overfitting to the large number of easy examples in pseudo labeled data. Our ST3D achieves state-of-the-art performance on all evaluated datasets and even surpasses fully supervised results on KITTI 3D object detection benchmark. Code will be available at https://github.com/CVMI-Lab/ST3D.},
  archiveprefix = {arXiv},
  eprint        = {2103.05346},
  file          = {:http\://arxiv.org/pdf/2103.05346v2:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@InProceedings{Chen2019a,
  author    = {Chen, Yuhua and Schmid, Cordelia and Sminchisescu, Cristian},
  booktitle = {CVPR},
  title     = {Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera},
  year      = {2019},
}

@Article{Loshchilov2017,
  author        = {Ilya Loshchilov and Frank Hutter},
  title         = {Decoupled Weight Decay Regularization},
  year          = {2017},
  month         = nov,
  abstract      = {L$_2$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \emph{not} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L$_2$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \emph{decoupling} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  archiveprefix = {arXiv},
  eprint        = {1711.05101},
  file          = {:http\://arxiv.org/pdf/1711.05101v3:PDF},
  keywords      = {cs.LG, cs.NE, math.OC},
  primaryclass  = {cs.LG},
}

@Article{Goyal2017,
  author        = {Priya Goyal and Piotr Dollár and Ross Girshick and Pieter Noordhuis and Lukasz Wesolowski and Aapo Kyrola and Andrew Tulloch and Yangqing Jia and Kaiming He},
  title         = {Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour},
  year          = {2017},
  month         = jun,
  abstract      = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archiveprefix = {arXiv},
  eprint        = {1706.02677},
  file          = {:http\://arxiv.org/pdf/1706.02677v2:PDF},
  keywords      = {cs.CV, cs.DC, cs.LG},
  primaryclass  = {cs.CV},
}

@Article{Loshchilov2016,
  author        = {Ilya Loshchilov and Frank Hutter},
  title         = {SGDR: Stochastic Gradient Descent with Warm Restarts},
  year          = {2016},
  month         = aug,
  abstract      = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  archiveprefix = {arXiv},
  eprint        = {1608.03983},
  file          = {:http\://arxiv.org/pdf/1608.03983v5:PDF},
  keywords      = {cs.LG, cs.NE, math.OC},
  primaryclass  = {cs.LG},
}

@Article{Chen2019b,
  author        = {Yuhua Chen and Cordelia Schmid and Cristian Sminchisescu},
  title         = {Self-supervised Learning with Geometric Constraints in Monocular Video: Connecting Flow, Depth, and Camera},
  year          = {2019},
  month         = jul,
  abstract      = {We present GLNet, a self-supervised framework for learning depth, optical flow, camera pose and intrinsic parameters from monocular video - addressing the difficulty of acquiring realistic ground-truth for such tasks. We propose three contributions: 1) we design new loss functions that capture multiple geometric constraints (eg. epipolar geometry) as well as an adaptive photometric loss that supports multiple moving objects, rigid and non-rigid, 2) we extend the model such that it predicts camera intrinsics, making it applicable to uncalibrated video, and 3) we propose several online refinement strategies that rely on the symmetry of our self-supervised loss in training and testing, in particular optimizing model parameters and/or the output of different tasks, thus leveraging their mutual interactions. The idea of jointly optimizing the system output, under all geometric and photometric constraints can be viewed as a dense generalization of classical bundle adjustment. We demonstrate the effectiveness of our method on KITTI and Cityscapes, where we outperform previous self-supervised approaches on multiple tasks. We also show good generalization for transfer learning in YouTube videos.},
  archiveprefix = {arXiv},
  eprint        = {1907.05820},
  file          = {:http\://arxiv.org/pdf/1907.05820v2:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
}

@Article{Wu2018,
  author        = {Zhirong Wu and Yuanjun Xiong and Stella Yu and Dahua Lin},
  title         = {Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination},
  year          = {2018},
  month         = may,
  abstract      = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
  archiveprefix = {arXiv},
  eprint        = {1805.01978},
  file          = {:http\://arxiv.org/pdf/1805.01978v1:PDF},
  keywords      = {cs.CV, cs.LG},
  primaryclass  = {cs.CV},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:SSL\;0\;0\;0x9df83cff\;\;\;;
1 StaticGroup:E2E Driving\;0\;1\;0xaf38e3ff\;\;\;;
1 StaticGroup:Misc\;0\;0\;0x808000ff\;\;\;;
}
