% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated as
% required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup

\datalist[entry]{nty/global//global/global}
  \entry{Ballas2015}{article}{}
    \name{author}{4}{}{%
      {{hash=BN}{%
         family={Ballas},
         familyi={B\bibinitperiod},
         given={Nicolas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=YL}{%
         family={Yao},
         familyi={Y\bibinitperiod},
         given={Li},
         giveni={L\bibinitperiod},
      }}%
      {{hash=PC}{%
         family={Pal},
         familyi={P\bibinitperiod},
         given={Chris},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CA}{%
         family={Courville},
         familyi={C\bibinitperiod},
         given={Aaron},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.LG, cs.NE}
    \strng{namehash}{BN+1}
    \strng{fullhash}{BNYLPCCA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    We propose an approach to learn spatio-temporal features in videos from
  intermediate visual representations we call "percepts" using
  Gated-Recurrent-Unit Recurrent Networks (GRUs).Our method relies on percepts
  that are extracted from all level of a deep convolutional network trained on
  the large ImageNet dataset. While high-level percepts contain highly
  discriminative information, they tend to have a low-spatial resolution.
  Low-level percepts, on the other hand, preserve a higher spatial resolution
  from which we can model finer motion patterns. Using low-level percepts can
  leads to high-dimensionality video representations. To mitigate this effect
  and control the model number of parameters, we introduce a variant of the GRU
  model that leverages the convolution operations to enforce sparse
  connectivity of the model units and share parameters across the input spatial
  locations. We empirically validate our approach on both Human Action
  Recognition and Video Captioning tasks. In particular, we achieve results
  equivalent to state-of-art on the YouTube2Text dataset using a simpler
  text-decoder model and without extra 3D CNN features.%
    }
    \verb{eprint}
    \verb 1511.06432
    \endverb
    \field{title}{Delving Deeper into Convolutional Networks for Learning Video
  Representations}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1511.06432v4:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{11}
    \field{year}{2015}
  \endentry

  \entry{Bansal2018}{article}{}
    \name{author}{3}{}{%
      {{hash=BM}{%
         family={Bansal},
         familyi={B\bibinitperiod},
         given={Mayank},
         giveni={M\bibinitperiod},
      }}%
      {{hash=KA}{%
         family={Krizhevsky},
         familyi={K\bibinitperiod},
         given={Alex},
         giveni={A\bibinitperiod},
      }}%
      {{hash=OA}{%
         family={Ogale},
         familyi={O\bibinitperiod},
         given={Abhijit},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.RO, cs.CV, cs.LG}
    \strng{namehash}{BMKAOA1}
    \strng{fullhash}{BMKAOA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    Our goal is to train a policy for autonomous driving via imitation learning
  that is robust enough to drive a real vehicle. We find that standard behavior
  cloning is insufficient for handling complex driving scenarios, even when we
  leverage a perception system for preprocessing the input and a controller for
  executing the output on the car: 30 million examples are still not enough. We
  propose exposing the learner to synthesized data in the form of perturbations
  to the expert's driving, which creates interesting situations such as
  collisions and/or going off the road. Rather than purely imitating all data,
  we augment the imitation loss with additional losses that penalize
  undesirable events and encourage progress -- the perturbations then provide
  an important signal for these losses and lead to robustness of the learned
  model. We show that the ChauffeurNet model can handle complex situations in
  simulation, and present ablation experiments that emphasize the importance of
  each of our proposed changes and show that the model is responding to the
  appropriate causal factors. Finally, we demonstrate the model driving a car
  in the real world.%
    }
    \verb{eprint}
    \verb 1812.03079
    \endverb
    \field{title}{ChauffeurNet: Learning to Drive by Imitating the Best and
  Synthesizing the Worst}
    \verb{file}
    \verb :Bansal2018 - ChauffeurNet_ Learning to Drive by Imitating the Best a
    \verb nd Synthesizing the Worst.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.RO}
    \field{month}{12}
    \field{year}{2018}
  \endentry

  \entry{10.2307/j.ctt183ph6v}{book}{}
    \name{author}{1}{}{%
      {{hash=BR}{%
         family={BELLMAN},
         familyi={B\bibinitperiod},
         given={RICHARD},
         giveni={R\bibinitperiod},
      }}%
    }
    \list{publisher}{1}{%
      {Princeton University Press}%
    }
    \strng{namehash}{BR1}
    \strng{fullhash}{BR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{B}
    \field{sortinithash}{B}
    \field{abstract}{%
    The book description for "Adaptive Control Processes" is currently
  unavailable.%
    }
    \field{isbn}{9780691079011}
    \field{title}{Adaptive Control Processes: A Guided Tour}
    \verb{url}
    \verb http://www.jstor.org/stable/j.ctt183ph6v
    \endverb
    \field{year}{1961}
    \field{urlday}{10}
    \field{urlmonth}{09}
    \field{urlyear}{2022}
  \endentry

  \entry{Caron2021}{article}{}
    \name{author}{7}{}{%
      {{hash=CM}{%
         family={Caron},
         familyi={C\bibinitperiod},
         given={Mathilde},
         giveni={M\bibinitperiod},
      }}%
      {{hash=TH}{%
         family={Touvron},
         familyi={T\bibinitperiod},
         given={Hugo},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MI}{%
         family={Misra},
         familyi={M\bibinitperiod},
         given={Ishan},
         giveni={I\bibinitperiod},
      }}%
      {{hash=JH}{%
         family={Jégou},
         familyi={J\bibinitperiod},
         given={Hervé},
         giveni={H\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Mairal},
         familyi={M\bibinitperiod},
         given={Julien},
         giveni={J\bibinitperiod},
      }}%
      {{hash=BP}{%
         family={Bojanowski},
         familyi={B\bibinitperiod},
         given={Piotr},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Joulin},
         familyi={J\bibinitperiod},
         given={Armand},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.CV}
    \strng{namehash}{CM+1}
    \strng{fullhash}{CMTHMIJHMJBPJA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In this paper, we question if self-supervised learning provides new
  properties to Vision Transformer (ViT) that stand out compared to
  convolutional networks (convnets). Beyond the fact that adapting
  self-supervised methods to this architecture works particularly well, we make
  the following observations: first, self-supervised ViT features contain
  explicit information about the semantic segmentation of an image, which does
  not emerge as clearly with supervised ViTs, nor with convnets. Second, these
  features are also excellent k-NN classifiers, reaching 78.3% top-1 on
  ImageNet with a small ViT. Our study also underlines the importance of
  momentum encoder, multi-crop training, and the use of small patches with
  ViTs. We implement our findings into a simple self-supervised method, called
  DINO, which we interpret as a form of self-distillation with no labels. We
  show the synergy between DINO and ViTs by achieving 80.1% top-1 on ImageNet
  in linear evaluation with ViT-Base.%
    }
    \verb{eprint}
    \verb 2104.14294
    \endverb
    \field{title}{Emerging Properties in Self-Supervised Vision Transformers}
    \verb{file}
    \verb :Caron2021 - Emerging Properties in Self Supervised Vision Transforme
    \verb rs.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{04}
    \field{year}{2021}
  \endentry

  \entry{Caron2020}{article}{}
    \name{author}{6}{}{%
      {{hash=CM}{%
         family={Caron},
         familyi={C\bibinitperiod},
         given={Mathilde},
         giveni={M\bibinitperiod},
      }}%
      {{hash=MI}{%
         family={Misra},
         familyi={M\bibinitperiod},
         given={Ishan},
         giveni={I\bibinitperiod},
      }}%
      {{hash=MJ}{%
         family={Mairal},
         familyi={M\bibinitperiod},
         given={Julien},
         giveni={J\bibinitperiod},
      }}%
      {{hash=GP}{%
         family={Goyal},
         familyi={G\bibinitperiod},
         given={Priya},
         giveni={P\bibinitperiod},
      }}%
      {{hash=BP}{%
         family={Bojanowski},
         familyi={B\bibinitperiod},
         given={Piotr},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JA}{%
         family={Joulin},
         familyi={J\bibinitperiod},
         given={Armand},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.CV}
    \strng{namehash}{CM+1}
    \strng{fullhash}{CMMIMJGPBPJA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Unsupervised image representations have significantly reduced the gap with
  supervised pretraining, notably with the recent achievements of contrastive
  learning methods. These contrastive methods typically work online and rely on
  a large number of explicit pairwise feature comparisons, which is
  computationally challenging. In this paper, we propose an online algorithm,
  SwAV, that takes advantage of contrastive methods without requiring to
  compute pairwise comparisons. Specifically, our method simultaneously
  clusters the data while enforcing consistency between cluster assignments
  produced for different augmentations (or views) of the same image, instead of
  comparing features directly as in contrastive learning. Simply put, we use a
  swapped prediction mechanism where we predict the cluster assignment of a
  view from the representation of another view. Our method can be trained with
  large and small batches and can scale to unlimited amounts of data. Compared
  to previous contrastive methods, our method is more memory efficient since it
  does not require a large memory bank or a special momentum network. In
  addition, we also propose a new data augmentation strategy, multi-crop, that
  uses a mix of views with different resolutions in place of two
  full-resolution views, without increasing the memory or compute requirements
  much. We validate our findings by achieving 75.3% top-1 accuracy on ImageNet
  with ResNet-50, as well as surpassing supervised pretraining on all the
  considered transfer tasks.%
    }
    \verb{eprint}
    \verb 2006.09882
    \endverb
    \field{title}{Unsupervised Learning of Visual Features by Contrasting
  Cluster Assignments}
    \verb{file}
    \verb :http\://arxiv.org/pdf/2006.09882v5:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{06}
    \field{year}{2020}
  \endentry

  \entry{Chen2019}{article}{}
    \name{author}{4}{}{%
      {{hash=CD}{%
         family={Chen},
         familyi={C\bibinitperiod},
         given={Dian},
         giveni={D\bibinitperiod},
      }}%
      {{hash=ZB}{%
         family={Zhou},
         familyi={Z\bibinitperiod},
         given={Brady},
         giveni={B\bibinitperiod},
      }}%
      {{hash=KV}{%
         family={Koltun},
         familyi={K\bibinitperiod},
         given={Vladlen},
         giveni={V\bibinitperiod},
      }}%
      {{hash=KP}{%
         family={Krähenbühl},
         familyi={K\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
    }
    \keyw{cs.RO, cs.AI, cs.CV, cs.LG}
    \strng{namehash}{CD+1}
    \strng{fullhash}{CDZBKVKP1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Vision-based urban driving is hard. The autonomous system needs to learn to
  perceive the world and act in it. We show that this challenging learning
  problem can be simplified by decomposing it into two stages. We first train
  an agent that has access to privileged information. This privileged agent
  cheats by observing the ground-truth layout of the environment and the
  positions of all traffic participants. In the second stage, the privileged
  agent acts as a teacher that trains a purely vision-based sensorimotor agent.
  The resulting sensorimotor agent does not have access to any privileged
  information and does not cheat. This two-stage training procedure is
  counter-intuitive at first, but has a number of important advantages that we
  analyze and empirically demonstrate. We use the presented approach to train a
  vision-based autonomous driving system that substantially outperforms the
  state of the art on the CARLA benchmark and the recent NoCrash benchmark. Our
  approach achieves, for the first time, 100% success rate on all tasks in the
  original CARLA benchmark, sets a new record on the NoCrash benchmark, and
  reduces the frequency of infractions by an order of magnitude compared to the
  prior state of the art. For the video that summarizes this work, see
  https://youtu.be/u9ZCxxD-UUw%
    }
    \verb{eprint}
    \verb 1912.12294
    \endverb
    \field{title}{Learning by Cheating}
    \verb{file}
    \verb :Chen2019 - Learning by Cheating.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.RO}
    \field{month}{12}
    \field{year}{2019}
  \endentry

  \entry{Chitta2021}{article}{}
    \name{author}{3}{}{%
      {{hash=CK}{%
         family={Chitta},
         familyi={C\bibinitperiod},
         given={Kashyap},
         giveni={K\bibinitperiod},
      }}%
      {{hash=PA}{%
         family={Prakash},
         familyi={P\bibinitperiod},
         given={Aditya},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Geiger},
         familyi={G\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.AI, cs.LG, cs.RO}
    \strng{namehash}{CKPAGA1}
    \strng{fullhash}{CKPAGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Efficient reasoning about the semantic, spatial, and temporal structure of
  a scene is a crucial prerequisite for autonomous driving. We present NEural
  ATtention fields (NEAT), a novel representation that enables such reasoning
  for end-to-end imitation learning models. NEAT is a continuous function which
  maps locations in Bird's Eye View (BEV) scene coordinates to waypoints and
  semantics, using intermediate attention maps to iteratively compress
  high-dimensional 2D image features into a compact representation. This allows
  our model to selectively attend to relevant regions in the input while
  ignoring information irrelevant to the driving task, effectively associating
  the images with the BEV representation. In a new evaluation setting involving
  adverse environmental conditions and challenging scenarios, NEAT outperforms
  several strong baselines and achieves driving scores on par with the
  privileged CARLA expert used to generate its training data. Furthermore,
  visualizing the attention maps for models with NEAT intermediate
  representations provides improved interpretability.%
    }
    \verb{eprint}
    \verb 2109.04456
    \endverb
    \field{title}{NEAT: Neural Attention Fields for End-to-End Autonomous
  Driving}
    \verb{file}
    \verb :Chitta2021 - NEAT_ Neural Attention Fields for End to End Autonomous
    \verb  Driving.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{09}
    \field{year}{2021}
  \endentry

  \entry{Cho2014}{article}{}
    \name{author}{7}{}{%
      {{hash=CK}{%
         family={Cho},
         familyi={C\bibinitperiod},
         given={Kyunghyun},
         giveni={K\bibinitperiod},
      }}%
      {{hash=vMB}{%
         prefix={van},
         prefixi={v\bibinitperiod},
         family={Merrienboer},
         familyi={M\bibinitperiod},
         given={Bart},
         giveni={B\bibinitperiod},
      }}%
      {{hash=GC}{%
         family={Gulcehre},
         familyi={G\bibinitperiod},
         given={Caglar},
         giveni={C\bibinitperiod},
      }}%
      {{hash=BD}{%
         family={Bahdanau},
         familyi={B\bibinitperiod},
         given={Dzmitry},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BF}{%
         family={Bougares},
         familyi={B\bibinitperiod},
         given={Fethi},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SH}{%
         family={Schwenk},
         familyi={S\bibinitperiod},
         given={Holger},
         giveni={H\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{cs.CL, cs.LG, cs.NE, stat.ML}
    \strng{namehash}{CK+1}
    \strng{fullhash}{CKMBvGCBDBFSHBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In this paper, we propose a novel neural network model called RNN
  Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN
  encodes a sequence of symbols into a fixed-length vector representation, and
  the other decodes the representation into another sequence of symbols. The
  encoder and decoder of the proposed model are jointly trained to maximize the
  conditional probability of a target sequence given a source sequence. The
  performance of a statistical machine translation system is empirically found
  to improve by using the conditional probabilities of phrase pairs computed by
  the RNN Encoder-Decoder as an additional feature in the existing log-linear
  model. Qualitatively, we show that the proposed model learns a semantically
  and syntactically meaningful representation of linguistic phrases.%
    }
    \verb{eprint}
    \verb 1406.1078
    \endverb
    \field{title}{Learning Phrase Representations using RNN Encoder-Decoder for
  Statistical Machine Translation}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1406.1078v3:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CL}
    \field{month}{06}
    \field{year}{2014}
  \endentry

  \entry{Chung2014}{article}{}
    \name{author}{4}{}{%
      {{hash=CJ}{%
         family={Chung},
         familyi={C\bibinitperiod},
         given={Junyoung},
         giveni={J\bibinitperiod},
      }}%
      {{hash=GC}{%
         family={Gulcehre},
         familyi={G\bibinitperiod},
         given={Caglar},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CK}{%
         family={Cho},
         familyi={C\bibinitperiod},
         given={KyungHyun},
         giveni={K\bibinitperiod},
      }}%
      {{hash=BY}{%
         family={Bengio},
         familyi={B\bibinitperiod},
         given={Yoshua},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{cs.NE, cs.LG}
    \strng{namehash}{CJ+1}
    \strng{fullhash}{CJGCCKBY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    In this paper we compare different types of recurrent units in recurrent
  neural networks (RNNs). Especially, we focus on more sophisticated units that
  implement a gating mechanism, such as a long short-term memory (LSTM) unit
  and a recently proposed gated recurrent unit (GRU). We evaluate these
  recurrent units on the tasks of polyphonic music modeling and speech signal
  modeling. Our experiments revealed that these advanced recurrent units are
  indeed better than more traditional recurrent units such as tanh units. Also,
  we found GRU to be comparable to LSTM.%
    }
    \verb{eprint}
    \verb 1412.3555
    \endverb
    \field{title}{Empirical Evaluation of Gated Recurrent Neural Networks on
  Sequence Modeling}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1412.3555v1:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.NE}
    \field{month}{12}
    \field{year}{2014}
  \endentry

  \entry{Codevilla2019}{article}{}
    \name{author}{4}{}{%
      {{hash=CF}{%
         family={Codevilla},
         familyi={C\bibinitperiod},
         given={Felipe},
         giveni={F\bibinitperiod},
      }}%
      {{hash=SE}{%
         family={Santana},
         familyi={S\bibinitperiod},
         given={Eder},
         giveni={E\bibinitperiod},
      }}%
      {{hash=LAM}{%
         family={López},
         familyi={L\bibinitperiod},
         given={Antonio\bibnamedelima M.},
         giveni={A\bibinitperiod\bibinitdelim M\bibinitperiod},
      }}%
      {{hash=GA}{%
         family={Gaidon},
         familyi={G\bibinitperiod},
         given={Adrien},
         giveni={A\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.AI}
    \strng{namehash}{CF+1}
    \strng{fullhash}{CFSELAMGA1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{C}
    \field{sortinithash}{C}
    \field{abstract}{%
    Driving requires reacting to a wide variety of complex environment
  conditions and agent behaviors. Explicitly modeling each possible scenario is
  unrealistic. In contrast, imitation learning can, in theory, leverage data
  from large fleets of human-driven cars. Behavior cloning in particular has
  been successfully used to learn simple visuomotor policies end-to-end, but
  scaling to the full spectrum of driving behaviors remains an unsolved
  problem. In this paper, we propose a new benchmark to experimentally
  investigate the scalability and limitations of behavior cloning. We show that
  behavior cloning leads to state-of-the-art results, including in unseen
  environments, executing complex lateral and longitudinal maneuvers without
  these reactions being explicitly programmed. However, we confirm well-known
  limitations (due to dataset bias and overfitting), new generalization issues
  (due to dynamic objects and the lack of a causal model), and training
  instability requiring further research before behavior cloning can graduate
  to real-world driving. The code of the studied behavior cloning approaches
  can be found at https://github.com/felipecode/coiltraine .%
    }
    \verb{eprint}
    \verb 1904.08980
    \endverb
    \field{title}{Exploring the Limitations of Behavior Cloning for Autonomous
  Driving}
    \verb{file}
    \verb :Codevilla2019 - Exploring the Limitations of Behavior Cloning for Au
    \verb tonomous Driving.pdf:PDF;:http\://arxiv.org/pdf/1904.08980v1:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{04}
    \field{year}{2019}
  \endentry

  \entry{Dosovitskiy2017}{article}{}
    \name{author}{5}{}{%
      {{hash=DA}{%
         family={Dosovitskiy},
         familyi={D\bibinitperiod},
         given={Alexey},
         giveni={A\bibinitperiod},
      }}%
      {{hash=RG}{%
         family={Ros},
         familyi={R\bibinitperiod},
         given={German},
         giveni={G\bibinitperiod},
      }}%
      {{hash=CF}{%
         family={Codevilla},
         familyi={C\bibinitperiod},
         given={Felipe},
         giveni={F\bibinitperiod},
      }}%
      {{hash=LA}{%
         family={Lopez},
         familyi={L\bibinitperiod},
         given={Antonio},
         giveni={A\bibinitperiod},
      }}%
      {{hash=KV}{%
         family={Koltun},
         familyi={K\bibinitperiod},
         given={Vladlen},
         giveni={V\bibinitperiod},
      }}%
    }
    \keyw{cs.LG, cs.AI, cs.CV, cs.RO}
    \strng{namehash}{DA+1}
    \strng{fullhash}{DARGCFLAKV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{D}
    \field{sortinithash}{D}
    \field{abstract}{%
    We introduce CARLA, an open-source simulator for autonomous driving
  research. CARLA has been developed from the ground up to support development,
  training, and validation of autonomous urban driving systems. In addition to
  open-source code and protocols, CARLA provides open digital assets (urban
  layouts, buildings, vehicles) that were created for this purpose and can be
  used freely. The simulation platform supports flexible specification of
  sensor suites and environmental conditions. We use CARLA to study the
  performance of three approaches to autonomous driving: a classic modular
  pipeline, an end-to-end model trained via imitation learning, and an
  end-to-end model trained via reinforcement learning. The approaches are
  evaluated in controlled scenarios of increasing difficulty, and their
  performance is examined via metrics provided by CARLA, illustrating the
  platform's utility for autonomous driving research. The supplementary video
  can be viewed at https://youtu.be/Hp8Dz-Zek2E%
    }
    \verb{eprint}
    \verb 1711.03938
    \endverb
    \field{title}{CARLA: An Open Urban Driving Simulator}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1711.03938v1:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{month}{11}
    \field{year}{2017}
  \endentry

  \entry{Filos2020}{article}{}
    \name{author}{6}{}{%
      {{hash=FA}{%
         family={Filos},
         familyi={F\bibinitperiod},
         given={Angelos},
         giveni={A\bibinitperiod},
      }}%
      {{hash=TP}{%
         family={Tigas},
         familyi={T\bibinitperiod},
         given={Panagiotis},
         giveni={P\bibinitperiod},
      }}%
      {{hash=MR}{%
         family={McAllister},
         familyi={M\bibinitperiod},
         given={Rowan},
         giveni={R\bibinitperiod},
      }}%
      {{hash=RN}{%
         family={Rhinehart},
         familyi={R\bibinitperiod},
         given={Nicholas},
         giveni={N\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Levine},
         familyi={L\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
      {{hash=GY}{%
         family={Gal},
         familyi={G\bibinitperiod},
         given={Yarin},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{cs.LG, cs.RO, stat.ML}
    \strng{namehash}{FA+1}
    \strng{fullhash}{FATPMRRNLSGY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Out-of-training-distribution (OOD) scenarios are a common challenge of
  learning agents at deployment, typically leading to arbitrary deductions and
  poorly-informed decisions. In principle, detection of and adaptation to OOD
  scenes can mitigate their adverse effects. In this paper, we highlight the
  limitations of current approaches to novel driving scenes and propose an
  epistemic uncertainty-aware planning method, called \emph{robust imitative
  planning} (RIP). Our method can detect and recover from some distribution
  shifts, reducing the overconfident and catastrophic extrapolations in OOD
  scenes. If the model's uncertainty is too great to suggest a safe course of
  action, the model can instead query the expert driver for feedback, enabling
  sample-efficient online adaptation, a variant of our method we term
  \emph{adaptive robust imitative planning} (AdaRIP). Our methods outperform
  current state-of-the-art approaches in the nuScenes \emph{prediction}
  challenge, but since no benchmark evaluating OOD detection and adaption
  currently exists to assess \emph{control}, we introduce an autonomous car
  novel-scene benchmark, \texttt{CARNOVEL}, to evaluate the robustness of
  driving agents to a suite of tasks with distribution shifts.%
    }
    \verb{eprint}
    \verb 2006.14911
    \endverb
    \field{title}{Can Autonomous Vehicles Identify, Recover From, and Adapt to
  Distribution Shifts?}
    \verb{file}
    \verb :http\://arxiv.org/pdf/2006.14911v2:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{month}{06}
    \field{year}{2020}
  \endentry

  \entry{Fischer2015}{article}{}
    \name{author}{9}{}{%
      {{hash=FP}{%
         family={Fischer},
         familyi={F\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={Dosovitskiy},
         familyi={D\bibinitperiod},
         given={Alexey},
         giveni={A\bibinitperiod},
      }}%
      {{hash=IE}{%
         family={Ilg},
         familyi={I\bibinitperiod},
         given={Eddy},
         giveni={E\bibinitperiod},
      }}%
      {{hash=HP}{%
         family={Häusser},
         familyi={H\bibinitperiod},
         given={Philip},
         giveni={P\bibinitperiod},
      }}%
      {{hash=HC}{%
         family={Hazırbaş},
         familyi={H\bibinitperiod},
         given={Caner},
         giveni={C\bibinitperiod},
      }}%
      {{hash=GV}{%
         family={Golkov},
         familyi={G\bibinitperiod},
         given={Vladimir},
         giveni={V\bibinitperiod},
      }}%
      {{hash=vdSP}{%
         prefix={van\bibnamedelima der},
         prefixi={v\bibinitperiod\bibinitdelim d\bibinitperiod},
         family={Smagt},
         familyi={S\bibinitperiod},
         given={Patrick},
         giveni={P\bibinitperiod},
      }}%
      {{hash=CD}{%
         family={Cremers},
         familyi={C\bibinitperiod},
         given={Daniel},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BT}{%
         family={Brox},
         familyi={B\bibinitperiod},
         given={Thomas},
         giveni={T\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.LG, I.2.6; I.4.8}
    \strng{namehash}{FP+1}
    \strng{fullhash}{FPDAIEHPHCGVSPvdCDBT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{F}
    \field{sortinithash}{F}
    \field{abstract}{%
    Convolutional neural networks (CNNs) have recently been very successful in
  a variety of computer vision tasks, especially on those linked to
  recognition. Optical flow estimation has not been among the tasks where CNNs
  were successful. In this paper we construct appropriate CNNs which are
  capable of solving the optical flow estimation problem as a supervised
  learning task. We propose and compare two architectures: a generic
  architecture and another one including a layer that correlates feature
  vectors at different image locations. Since existing ground truth data sets
  are not sufficiently large to train a CNN, we generate a synthetic Flying
  Chairs dataset. We show that networks trained on this unrealistic data still
  generalize very well to existing datasets such as Sintel and KITTI, achieving
  competitive accuracy at frame rates of 5 to 10 fps.%
    }
    \verb{eprint}
    \verb 1504.06852
    \endverb
    \field{title}{FlowNet: Learning Optical Flow with Convolutional Networks}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1504.06852v2:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{04}
    \field{year}{2015}
  \endentry

  \entry{Geiger2012AreWR}{article}{}
    \name{author}{3}{}{%
      {{hash=GA}{%
         family={Geiger},
         familyi={G\bibinitperiod},
         given={Andreas},
         giveni={A\bibinitperiod},
      }}%
      {{hash=LP}{%
         family={Lenz},
         familyi={L\bibinitperiod},
         given={Philip},
         giveni={P\bibinitperiod},
      }}%
      {{hash=UR}{%
         family={Urtasun},
         familyi={U\bibinitperiod},
         given={Raquel},
         giveni={R\bibinitperiod},
      }}%
    }
    \strng{namehash}{GALPUR1}
    \strng{fullhash}{GALPUR1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{G}
    \field{sortinithash}{G}
    \field{pages}{3354\bibrangedash 3361}
    \field{title}{Are we ready for autonomous driving? The KITTI vision
  benchmark suite}
    \field{journaltitle}{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}
    \field{year}{2012}
  \endentry

  \entry{Haan2019}{article}{}
    \name{author}{3}{}{%
      {{hash=dHP}{%
         prefix={de},
         prefixi={d\bibinitperiod},
         family={Haan},
         familyi={H\bibinitperiod},
         given={Pim},
         giveni={P\bibinitperiod},
      }}%
      {{hash=JD}{%
         family={Jayaraman},
         familyi={J\bibinitperiod},
         given={Dinesh},
         giveni={D\bibinitperiod},
      }}%
      {{hash=LS}{%
         family={Levine},
         familyi={L\bibinitperiod},
         given={Sergey},
         giveni={S\bibinitperiod},
      }}%
    }
    \keyw{cs.LG, stat.ML}
    \strng{namehash}{HPdJDLS1}
    \strng{fullhash}{HPdJDLS1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{H}
    \field{sortinithash}{H}
    \field{abstract}{%
    Behavioral cloning reduces policy learning to supervised learning by
  training a discriminative model to predict expert actions given observations.
  Such discriminative models are non-causal: the training procedure is unaware
  of the causal structure of the interaction between the expert and the
  environment. We point out that ignoring causality is particularly damaging
  because of the distributional shift in imitation learning. In particular, it
  leads to a counter-intuitive "causal misidentification" phenomenon: access to
  more information can yield worse performance. We investigate how this problem
  arises, and propose a solution to combat it through targeted
  interventions---either environment interaction or expert queries---to
  determine the correct causal model. We show that causal misidentification
  occurs in several benchmark control domains as well as realistic driving
  settings, and validate our solution against DAgger and other baselines and
  ablations.%
    }
    \verb{eprint}
    \verb 1905.11979
    \endverb
    \field{title}{Causal Confusion in Imitation Learning}
    \verb{file}
    \verb :Haan2019 - Causal Confusion in Imitation Learning.pdf:PDF;:http\://a
    \verb rxiv.org/pdf/1905.11979v2:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.LG}
    \field{month}{05}
    \field{year}{2019}
  \endentry

  \entry{Muller2006}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=MU}{%
         family={Muller},
         familyi={M\bibinitperiod},
         given={Urs},
         giveni={U\bibinitperiod},
      }}%
      {{hash=BJ}{%
         family={Ben},
         familyi={B\bibinitperiod},
         given={Jan},
         giveni={J\bibinitperiod},
      }}%
      {{hash=CE}{%
         family={Cosatto},
         familyi={C\bibinitperiod},
         given={Eric},
         giveni={E\bibinitperiod},
      }}%
      {{hash=FB}{%
         family={Flepp},
         familyi={F\bibinitperiod},
         given={Beat},
         giveni={B\bibinitperiod},
      }}%
      {{hash=CYL}{%
         family={Cun},
         familyi={C\bibinitperiod},
         given={Yann\bibnamedelima L},
         giveni={Y\bibinitperiod\bibinitdelim L\bibinitperiod},
      }}%
    }
    \strng{namehash}{MU+1}
    \strng{fullhash}{MUBJCEFBCYL1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{booktitle}{NeurIPS}
    \field{title}{Off-road obstacle avoidance through end-to-end learning}
    \field{year}{2006}
  \endentry

  \entry{Mueller2018}{article}{}
    \name{author}{4}{}{%
      {{hash=MM}{%
         family={Müller},
         familyi={M\bibinitperiod},
         given={Matthias},
         giveni={M\bibinitperiod},
      }}%
      {{hash=DA}{%
         family={Dosovitskiy},
         familyi={D\bibinitperiod},
         given={Alexey},
         giveni={A\bibinitperiod},
      }}%
      {{hash=GB}{%
         family={Ghanem},
         familyi={G\bibinitperiod},
         given={Bernard},
         giveni={B\bibinitperiod},
      }}%
      {{hash=KV}{%
         family={Koltun},
         familyi={K\bibinitperiod},
         given={Vladlen},
         giveni={V\bibinitperiod},
      }}%
    }
    \keyw{cs.RO, cs.CV, cs.LG}
    \strng{namehash}{MM+1}
    \strng{fullhash}{MMDAGBKV1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{M}
    \field{sortinithash}{M}
    \field{abstract}{%
    End-to-end approaches to autonomous driving have high sample complexity and
  are difficult to scale to realistic urban driving. Simulation can help
  end-to-end driving systems by providing a cheap, safe, and diverse training
  environment. Yet training driving policies in simulation brings up the
  problem of transferring such policies to the real world. We present an
  approach to transferring driving policies from simulation to reality via
  modularity and abstraction. Our approach is inspired by classic driving
  systems and aims to combine the benefits of modular architectures and
  end-to-end deep learning approaches. The key idea is to encapsulate the
  driving policy such that it is not directly exposed to raw perceptual input
  or low-level vehicle dynamics. We evaluate the presented approach in
  simulated urban environments and in the real world. In particular, we
  transfer a driving policy trained in simulation to a 1/5-scale robotic truck
  that is deployed in a variety of conditions, with no finetuning, on two
  continents. The supplementary video can be viewed at
  https://youtu.be/BrMDJqI6H5U%
    }
    \verb{eprint}
    \verb 1804.09364
    \endverb
    \field{title}{Driving Policy Transfer via Modularity and Abstraction}
    \verb{file}
    \verb :http\://arxiv.org/pdf/1804.09364v3:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.RO}
    \field{month}{04}
    \field{year}{2018}
  \endentry

  \entry{Wang2019a}{inproceedings}{}
    \name{author}{5}{}{%
      {{hash=WD}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Dequan},
         giveni={D\bibinitperiod},
      }}%
      {{hash=DC}{%
         family={Devin},
         familyi={D\bibinitperiod},
         given={Coline},
         giveni={C\bibinitperiod},
      }}%
      {{hash=CQZ}{%
         family={Cai},
         familyi={C\bibinitperiod},
         given={Qi-Zhi},
         giveni={Q\bibinithyphendelim Z\bibinitperiod},
      }}%
      {{hash=KP}{%
         family={Kr{\"a}henb{\"u}hl},
         familyi={K\bibinitperiod},
         given={Philipp},
         giveni={P\bibinitperiod},
      }}%
      {{hash=DT}{%
         family={Darrell},
         familyi={D\bibinitperiod},
         given={Trevor},
         giveni={T\bibinitperiod},
      }}%
    }
    \strng{namehash}{WD+1}
    \strng{fullhash}{WDDCCQZKPDT1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{booktitle}{IROS}
    \field{title}{Monocular plan view networks for autonomous driving}
    \field{year}{2019}
  \endentry

  \entry{Wang2017}{article}{}
    \name{author}{4}{}{%
      {{hash=WS}{%
         family={Wang},
         familyi={W\bibinitperiod},
         given={Sen},
         giveni={S\bibinitperiod},
      }}%
      {{hash=CR}{%
         family={Clark},
         familyi={C\bibinitperiod},
         given={Ronald},
         giveni={R\bibinitperiod},
      }}%
      {{hash=WH}{%
         family={Wen},
         familyi={W\bibinitperiod},
         given={Hongkai},
         giveni={H\bibinitperiod},
      }}%
      {{hash=TN}{%
         family={Trigoni},
         familyi={T\bibinitperiod},
         given={Niki},
         giveni={N\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.RO}
    \strng{namehash}{WS+1}
    \strng{fullhash}{WSCRWHTN1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{abstract}{%
    This paper studies monocular visual odometry (VO) problem. Most of existing
  VO algorithms are developed under a standard pipeline including feature
  extraction, feature matching, motion estimation, local optimisation, etc.
  Although some of them have demonstrated superior performance, they usually
  need to be carefully designed and specifically fine-tuned to work well in
  different environments. Some prior knowledge is also required to recover an
  absolute scale for monocular VO. This paper presents a novel end-to-end
  framework for monocular VO by using deep Recurrent Convolutional Neural
  Networks (RCNNs). Since it is trained and deployed in an end-to-end manner,
  it infers poses directly from a sequence of raw RGB images (videos) without
  adopting any module in the conventional VO pipeline. Based on the RCNNs, it
  not only automatically learns effective feature representation for the VO
  problem through Convolutional Neural Networks, but also implicitly models
  sequential dynamics and relations using deep Recurrent Neural Networks.
  Extensive experiments on the KITTI VO dataset show competitive performance to
  state-of-the-art methods, verifying that the end-to-end Deep Learning
  technique can be a viable complement to the traditional VO systems.%
    }
    \verb{doi}
    \verb 10.1109/ICRA.2017.7989236
    \endverb
    \verb{eprint}
    \verb 1709.08429
    \endverb
    \field{title}{DeepVO: Towards End-to-End Visual Odometry with Deep
  Recurrent Convolutional Neural Networks}
    \verb{file}
    \verb :Wang2017 - DeepVO_ Towards End to End Visual Odometry with Deep Recu
    \verb rrent Convolutional Neural Networks.pdf:PDF;:http\://arxiv.org/pdf/17
    \verb 09.08429v1:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{09}
    \field{year}{2017}
  \endentry

  \entry{Argoverse2}{inproceedings}{}
    \name{author}{13}{}{%
      {{hash=WB}{%
         family={Wilson},
         familyi={W\bibinitperiod},
         given={Benjamin},
         giveni={B\bibinitperiod},
      }}%
      {{hash=QW}{%
         family={Qi},
         familyi={Q\bibinitperiod},
         given={William},
         giveni={W\bibinitperiod},
      }}%
      {{hash=AT}{%
         family={Agarwal},
         familyi={A\bibinitperiod},
         given={Tanmay},
         giveni={T\bibinitperiod},
      }}%
      {{hash=LJ}{%
         family={Lambert},
         familyi={L\bibinitperiod},
         given={John},
         giveni={J\bibinitperiod},
      }}%
      {{hash=SJ}{%
         family={Singh},
         familyi={S\bibinitperiod},
         given={Jagjeet},
         giveni={J\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Khandelwal},
         familyi={K\bibinitperiod},
         given={Siddhesh},
         giveni={S\bibinitperiod},
      }}%
      {{hash=PB}{%
         family={Pan},
         familyi={P\bibinitperiod},
         given={Bowen},
         giveni={B\bibinitperiod},
      }}%
      {{hash=KR}{%
         family={Kumar},
         familyi={K\bibinitperiod},
         given={Ratnesh},
         giveni={R\bibinitperiod},
      }}%
      {{hash=HA}{%
         family={Hartnett},
         familyi={H\bibinitperiod},
         given={Andrew},
         giveni={A\bibinitperiod},
      }}%
      {{hash=PJK}{%
         family={Pontes},
         familyi={P\bibinitperiod},
         given={Jhony\bibnamedelima Kaesemodel},
         giveni={J\bibinitperiod\bibinitdelim K\bibinitperiod},
      }}%
      {{hash=RD}{%
         family={Ramanan},
         familyi={R\bibinitperiod},
         given={Deva},
         giveni={D\bibinitperiod},
      }}%
      {{hash=CP}{%
         family={Carr},
         familyi={C\bibinitperiod},
         given={Peter},
         giveni={P\bibinitperiod},
      }}%
      {{hash=HJ}{%
         family={Hays},
         familyi={H\bibinitperiod},
         given={James},
         giveni={J\bibinitperiod},
      }}%
    }
    \strng{namehash}{WB+1}
    \strng{fullhash}{WBQWATLJSJKSPBKRHAPJKRDCPHJ1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{W}
    \field{sortinithash}{W}
    \field{booktitle}{Proceedings of the Neural Information Processing Systems
  Track on Datasets and Benchmarks (NeurIPS Datasets and Benchmarks 2021)}
    \field{title}{Argoverse 2: Next Generation Datasets for Self-driving
  Perception and Forecasting}
    \field{year}{2021}
  \endentry

  \entry{Yadav2022}{article}{}
    \name{author}{8}{}{%
      {{hash=YK}{%
         family={Yadav},
         familyi={Y\bibinitperiod},
         given={Karmesh},
         giveni={K\bibinitperiod},
      }}%
      {{hash=RR}{%
         family={Ramrakhya},
         familyi={R\bibinitperiod},
         given={Ram},
         giveni={R\bibinitperiod},
      }}%
      {{hash=MA}{%
         family={Majumdar},
         familyi={M\bibinitperiod},
         given={Arjun},
         giveni={A\bibinitperiod},
      }}%
      {{hash=BVP}{%
         family={Berges},
         familyi={B\bibinitperiod},
         given={Vincent-Pierre},
         giveni={V\bibinithyphendelim P\bibinitperiod},
      }}%
      {{hash=KS}{%
         family={Kuhar},
         familyi={K\bibinitperiod},
         given={Sachit},
         giveni={S\bibinitperiod},
      }}%
      {{hash=BD}{%
         family={Batra},
         familyi={B\bibinitperiod},
         given={Dhruv},
         giveni={D\bibinitperiod},
      }}%
      {{hash=BA}{%
         family={Baevski},
         familyi={B\bibinitperiod},
         given={Alexei},
         giveni={A\bibinitperiod},
      }}%
      {{hash=MO}{%
         family={Maksymets},
         familyi={M\bibinitperiod},
         given={Oleksandr},
         giveni={O\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.LG}
    \strng{namehash}{YK+1}
    \strng{fullhash}{YKRRMABVPKSBDBAMO1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Y}
    \field{sortinithash}{Y}
    \field{abstract}{%
    How should we learn visual representations for embodied agents that must
  see and move? The status quo is tabula rasa in vivo, i.e. learning visual
  representations from scratch while also learning to move, potentially
  augmented with auxiliary tasks (e.g. predicting the action taken between two
  successive observations). In this paper, we show that an alternative 2-stage
  strategy is far more effective: (1) offline pretraining of visual
  representations with self-supervised learning (SSL) using large-scale
  pre-rendered images of indoor environments (Omnidata), and (2) online
  finetuning of visuomotor representations on specific tasks with image
  augmentations under long learning schedules. We call this method Offline
  Visual Representation Learning (OVRL). We conduct large-scale experiments -
  on 3 different 3D datasets (Gibson, HM3D, MP3D), 2 tasks (ImageNav,
  ObjectNav), and 2 policy learning algorithms (RL, IL) - and find that the
  OVRL representations lead to significant across-the-board improvements in
  state of art, on ImageNav from 29.2% to 54.2% (+25% absolute, 86% relative)
  and on ObjectNav from 18.1% to 23.2% (+5.1% absolute, 28% relative).
  Importantly, both results were achieved by the same visual encoder
  generalizing to datasets that were not seen during pretraining. While the
  benefits of pretraining sometimes diminish (or entirely disappear) with long
  finetuning schedules, we find that OVRL's performance gains continue to
  increase (not decrease) as the agent is trained for 2 billion frames of
  experience.%
    }
    \verb{eprint}
    \verb 2204.13226
    \endverb
    \field{title}{Offline Visual Representation Learning for Embodied
  Navigation}
    \verb{file}
    \verb :Yadav2022 - Offline Visual Representation Learning for Embodied Navi
    \verb gation.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{04}
    \field{year}{2022}
  \endentry

  \entry{Zhai2019}{article}{}
    \name{author}{4}{}{%
      {{hash=ZG}{%
         family={Zhai},
         familyi={Z\bibinitperiod},
         given={Guangyao},
         giveni={G\bibinitperiod},
      }}%
      {{hash=LL}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Liang},
         giveni={L\bibinitperiod},
      }}%
      {{hash=ZL}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Linjian},
         giveni={L\bibinitperiod},
      }}%
      {{hash=LY}{%
         family={Liu},
         familyi={L\bibinitperiod},
         given={Yong},
         giveni={Y\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.LG, eess.IV}
    \strng{namehash}{ZG+1}
    \strng{fullhash}{ZGLLZLLY1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    While many visual ego-motion algorithm variants have been proposed in the
  past decade, learning based ego-motion estimation methods have seen an
  increasing attention because of its desirable properties of robustness to
  image noise and camera calibration independence. In this work, we propose a
  data-driven approach of fully trainable visual ego-motion estimation for a
  monocular camera. We use an end-to-end learning approach in allowing the
  model to map directly from input image pairs to an estimate of ego-motion
  (parameterized as 6-DoF transformation matrices). We introduce a novel
  two-module Long-term Recurrent Convolutional Neural Networks called
  PoseConvGRU, with an explicit sequence pose estimation loss to achieve this.
  The feature-encoding module encodes the short-term motion feature in an image
  pair, while the memory-propagating module captures the long-term motion
  feature in the consecutive image pairs. The visual memory is implemented with
  convolutional gated recurrent units, which allows propagating information
  over time. At each time step, two consecutive RGB images are stacked together
  to form a 6 channels tensor for module-1 to learn how to extract motion
  information and estimate poses. The sequence of output maps is then passed
  through a stacked ConvGRU module to generate the relative transformation pose
  of each image pair. We also augment the training data by randomly skipping
  frames to simulate the velocity variation which results in a better
  performance in turning and high-velocity situations. We evaluate the
  performance of our proposed approach on the KITTI Visual Odometry benchmark.
  The experiments show a competitive performance of the proposed method to the
  geometric method and encourage further exploration of learning based methods
  for the purpose of estimating camera ego-motion even though geometrical
  methods demonstrate promising results.%
    }
    \verb{eprint}
    \verb 1906.08095
    \endverb
    \field{title}{PoseConvGRU: A Monocular Approach for Visual Ego-motion
  Estimation by Learning}
    \verb{file}
    \verb :Zhai2019 - PoseConvGRU_ a Monocular Approach for Visual Ego Motion E
    \verb stimation by Learning.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{06}
    \field{year}{2019}
  \endentry

  \entry{Zhang2022a}{article}{}
    \name{author}{3}{}{%
      {{hash=ZJ}{%
         family={Zhang},
         familyi={Z\bibinitperiod},
         given={Jimuyang},
         giveni={J\bibinitperiod},
      }}%
      {{hash=ZR}{%
         family={Zhu},
         familyi={Z\bibinitperiod},
         given={Ruizhao},
         giveni={R\bibinitperiod},
      }}%
      {{hash=OBE}{%
         family={Ohn-Bar},
         familyi={O\bibinithyphendelim B\bibinitperiod},
         given={Eshed},
         giveni={E\bibinitperiod},
      }}%
    }
    \keyw{cs.CV, cs.AI, cs.RO}
    \strng{namehash}{ZJZROBE1}
    \strng{fullhash}{ZJZROBE1}
    \field{labelnamesource}{author}
    \field{labeltitlesource}{title}
    \field{sortinit}{Z}
    \field{sortinithash}{Z}
    \field{abstract}{%
    Effectively utilizing the vast amounts of ego-centric navigation data that
  is freely available on the internet can advance generalized intelligent
  systems, i.e., to robustly scale across perspectives, platforms,
  environmental conditions, scenarios, and geographical locations. However, it
  is difficult to directly leverage such large amounts of unlabeled and highly
  diverse data for complex 3D reasoning and planning tasks. Consequently,
  researchers have primarily focused on its use for various auxiliary pixel-
  and image-level computer vision tasks that do not consider an ultimate
  navigational objective. In this work, we introduce SelfD, a framework for
  learning scalable driving by utilizing large amounts of online monocular
  images. Our key idea is to leverage iterative semi-supervised training when
  learning imitative agents from unlabeled data. To handle unconstrained
  viewpoints, scenes, and camera parameters, we train an image-based model that
  directly learns to plan in the Bird's Eye View (BEV) space. Next, we use
  unlabeled data to augment the decision-making knowledge and robustness of an
  initially trained model via self-training. In particular, we propose a
  pseudo-labeling step which enables making full use of highly diverse
  demonstration data through "hypothetical" planning-based data augmentation.
  We employ a large dataset of publicly available YouTube videos to train SelfD
  and comprehensively analyze its generalization benefits across challenging
  navigation scenarios. Without requiring any additional data collection or
  annotation efforts, SelfD demonstrates consistent improvements (by up to 24%)
  in driving performance evaluation on nuScenes, Argoverse, Waymo, and CARLA.%
    }
    \verb{eprint}
    \verb 2204.10320
    \endverb
    \field{title}{SelfD: Self-Learning Large-Scale Driving Policies From the
  Web}
    \verb{file}
    \verb :Zhang2022a - SelfD_ Self Learning Large Scale Driving Policies from
    \verb the Web.pdf:PDF
    \endverb
    \field{eprinttype}{arXiv}
    \field{eprintclass}{cs.CV}
    \field{month}{04}
    \field{year}{2022}
  \endentry
\enddatalist
\endinput
