#+LaTeX_CLASS: article
#+LaTeX_CLASS_OPTIONS: [letterpaper, 12pt]
#+TITLE: Exploring Supervised and Self-supervised Learning in Autonomous Driving 
#+SUBTITLE: 
#+DESCRIPTION:
#+EMAIL:
#+DATE: 
#+KEYWORDS:
#+OPTIONS: toc:nil
#+OPTIONS: H:5 num:t
#+SETUPFILE: ~/Templates/org/headers.org
#+LATEX_HEADER: \usepackage[left=3cm,right=3cm,top=3cm,bottom=4cm]{geometry}
#+LATEX_HEADER: \usepackage[backend=bibtex]{biblatex}
#+LATEX_HEADER: \addbibresource{../papers/thesis.bib}

#+LATEX_HEADER: \author{Partha Ghosh\\University of Tübingen}
#+LATEX_HEADER: \usepackage[parfill]{parskip}

#+LATEX_HEADER: \sethlcolor{Goldenrod}\renewcommand{\sout}{\hl}
#+LATEX_HEADER: \usepackage{fontspec}\setmainfont{Times New Roman}

#+LATEX_HEADER: \makeatletter \newcommand\subsubsubsection{\@startsection{paragraph}{4}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}} \newcommand\subsubsubsubsection{\@startsection{subparagraph}{5}{\z@}{-2.5ex\@plus -1ex \@minus -.25ex}{1.25ex \@plus .25ex}{\normalfont\normalsize\bfseries}} \makeatother


#+LATEX_HEADER: \setcounter{secnumdepth}{5} \setcounter{tocdepth}{10}

#+LATEX_HEADER: \newcommand{\vth}{\v{\th}}

#+BEGIN_SRC emacs-lisp :exports none :results none
  (org-latex-export-to-latex)
  (shell-command-to-string (concat "de-macro " (file-name-base (buffer-file-name)) ".tex && latexmk -bibtex-cond1 -pdflua " (file-name-base (buffer-file-name)) "-clean.tex"))
  (add-to-list 'org-latex-classes
             '("article"
               "\\documentclass{article}"
               ("\\section{%s}" . "\\section*{%s}")
               ("\\subsection{%s}" . "\\subsection*{%s}")
               ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
               ("\\subsubsubsection{%s}" . "\\subsubsubsection*{%s}")
               ("\\subsubsubsubsection{%s}" . "\\subsubsubsubsection*{%s}")))
#+END_SRC

#+BEGIN_COMMENT

- C-x C; to comment a line
- Just get started don't procrastinate.
- Create an outline.
- Do not write the introduction in the beginning.
- Begin with the experimental section.
- Now write the results and discussion.
- Write the conclusion (in numbered format)
- Write the abstract and acknowledgements after conclusion
- Write the introduction
  - What is the purpose of this study
  - Give sufficient background for the reader to understand the study

https://pubs.acs.org/doi/pdf/10.1021/ac2000169

- Explain the objective of the paper in a single sentence at the beginning of
the paper
- Describe the experiment(s) in a single sentence or paragraph at the
beginning. Be clear and explicit with essential details and why the experiment
was successfully provide the information in this context.
- Diagrams should be explain properly with labels.
- Don't use abbreviations.
- Use your personality, phrasing (don't be very stiff and formal).
- Explain the reasoning in the interpretation of the results. Point out their
impact or consistency with other author's results and interpretation (don't make
it as an island of original thinking).

#+END_COMMENT


* COMMENT Thesis Content



** Method

*** Problem Setting

**** 
** Experiment

In this section we will describe our evaluation methodology, introduce the baselines and discuss results.
*** Task
* COMMENT Thesis Progression
- [0/5]
  - [-] Introduction [0/8]
    - [ ] What is SSL?
    - [ ] What is Autonomous Driving?
    - [ ] What is the problem?
    - [ ] Why SSL important in Autonomous Driving?
    - [ ] Does SSL works in Autonomous Driving?
    - [ ] Goal is to investigate SSL in Autonomous Driving
    - [ ] Proposed a new SSL algorithm more robust
    - [-] Organization of the rest of the thesis
      - [X] Boiler plate content
      - [ ] Still need to modify
  - [ ] Related Works [0/2]
    - [ ] Self-supervised Learning [0/4]
      - [ ] Emerging Properties in Self-Supervised Vision Transformers
      - [ ] Exploring Simple Siamese Representation Learning
      - [ ] Momentum Contrast for Unsupervised Visual Representation Learning
      - [ ] Self-training with Noisy Student improves ImageNet classification
    - [ ] Self-supervised Learning in Autonomous Driving [0/3]
      - [ ] SelfD: Self-Learning Large-Scale Driving Policies From the Web
      - [ ] Offline Visual Representation Learning for Embodied Navigation
      - [ ] Action-Conditioned Contrastive Policy Pretraining
  - [ ] Method [0/2]
    - [ ] Method
    - [ ] Implementation Details [0/0]
  - [-] Experimental Results [1/3]
    - [X] Evaluation Metrics
    - [ ] Baselines
    - [ ] Ablation Studies
  - [ ] Conclusion
* COMMENT Info
:PROPERTIES:
:UNNUMBERED: t
:END:

- SelfD is trash
  - Confidence score is better for straight driving not good for curbs
  - After filtering only straight waypoints remain
- CILRS is trash
- Goal location is essential for good driving performance
- Data augmentation is essential for good driving performance
- Once you learned to believe wrong things, it becomes hard to correct them
- At the turns:
  - For the training towns, the pseudo-waypoints are close to the GT waypoints
    (the predicted confidence score is above 0.33)
  - For the other towns, the predicted confidence score of the pseudo-waypoints
    are below 0.33 and therefore have been filtered out
  - For the other towns, the calculated confidence score of the pseudo-waypoints
    are below 0.33 and therefore have been filtered out
  - This implies the confidence score prediction works reasonably well. But the
    problem is network could not predict very accurately at the turns, which is
    essential for driving.
- 14_minimal_weather dataset has some faulty demostrations (running into
  vehicle, pedestrian etc., traffic light is not visible int the screen, scenes
  did not rendered as expected)
- Self-supervised learning is not helpful in the paradigm advocated in SelfD.
- Driving is a safety-critical task and therefore pseudo-labels have to be good
  enough to initialize the network.
- Using DeepVO we can generate accurate enough pseudo-labels especially at the
  turns. Therefore, we can use these pseudo-labels for Self-supervised training.
- Because of spurious correlations, sometimes the car would not start or would
  not start after it stops.
- If a car is infront a bit far, whether the ego vehicle should accelerate or
  deaccelerate is confusing. Therefore, we also have to input whether the ego
  vehicle was accelerating or deaccelerating in the previous timestep as input.
- Images are too small, sometimes the traffic light is not reconizable.
- Pedestrian coming after the vehicle already past and then collide with the
  vehicle, or ego-vehicle turns/chnages lane first but the car behind could not
  stop and therefore make a slight collision
- Talk about why we didn't consider real dataset:
  - the cause of high infraction score in the first 2 dataset is beacuse the red
    lights are not visible
  - in the pami datset they are visible but it uses middle crop -> the
    corresponding crop from real data can be hard to obtain



Results:
- SelfD is not reproducible, in fact performance the model worsen after training
  with pseudo-labels.
- SelfD is a self-supervised learning framework, therefore it should be model
  agnostic. Therefore, even if we do not use the exact model, we should expect
  similar or better result after training with pseudo-labels.

Story:
- we saw with navigational command driving performance is not satisfactory
- the authors of selfd used navigational command for
- imgaug helps in driving
- occsionally the car would stop because of the additional info need like acceleration
- We also solved the 
- we also found that speed information may be beneficial for driving, a progress
  towards solving the causal confusion problem, but more
  importantly the information of car acceleration bring tremendous improvement
  in driving performance. Because to assess, what
- we have seen the only kind of infractions that are happening are redlight
  violations. this is simply because the images are too small and therefore
  sometimes the traffic light is not recognizable. This mostly happens at the
  juction point of highways.
- The stratified sampling is not complete, it cannot sample different modes of
  stops, for example, we do not know whether the car stopped because of a car,
  red light or a pedestrian. Also we do not sample whether car took a left turn
  or right turn. evenly sampling these modalities can also help improving the
  performance of the model. To mitigate long break, short break issue we just
  consider only few frames at the beginning and at the end when the ego vehicle
  is at stop.

* COMMENT Useful Info

  - We organize the structure of the paper as follows. We first provide an
    overview of related studies in the literature in Sec. 2; we then introduce
    details of our methodology in Sec. 3, followed by an empirical assessment of
    the effectiveness of our model in Sec. 4. Finally, we evaluate the design
    choices of our kernel parameterization scheme in Sec. 5.
  - https://en.wikipedia.org/wiki/Self-supervised_learning
 
* Abstract
:PROPERTIES:
:UNNUMBERED: t
:END:


@@latex:\clearpage@@ 

* Acknowledgements
:PROPERTIES:
:UNNUMBERED: t
:END:

#+BEGIN_COMMENT
  The first big thank goes to my family, who has always encouraged me and
supported me in my choices. The most important and difficult present you
can give to your son is to let him be the builder of his own life.
I want to reserve a second, very special thank, to my professor Frederic
Precioso, firstly, for the amazing person that he is, secondly for the incredible
professor and tutor that he has been. This thesis would have never been
the same without him, and more importantly, it would have never been that
funny.
A really heartfelt thank you goes to my industrial tutor Christelle Yemdji
Tchassi who welcomed me in Renault in the best of the ways. I couldn’t
have asked for someone more available and kinder than her.
I want to thank also professor Matteo Matteucci, for being my supervisor
in Politecnico di Milano and for the feedbacks he gave me on my work.
A big thank you goes to the ASL team who introduced me to deep
learning, without them I would have never discovered my passion for this
subject, and I would have never completed this work.
Finally, I extend my thanks to all the friends I met in the university and
in Nice this year, who sustained me in all the moments of difficulties and
encouraged me to get to the end of this work.

---------

I would firstly like to thank my advisor, Daniela Rus, for her incredible support and
guidance throughout this thesis. I would also like to thank the remainder of MIT's
Distributed Robotics Laboratory for their insightful discussions.
Finally, I must express my very profound gratitude to my parents, Sean and Lisa
Amini, my siblings, and my partner, Ava Soleimany, for providing me with unfailing
support and continuous encouragement throughout my years of study and tihrough
the process of researching and writing this thesis.
This accomplishment would not have been possible without them.

#+END_COMMENT


  I want to thank Bernhard Jaeger and Katrin Renz for supervising the project
  and providing me with the necessary guidance and valuable support. I want to
  thank Prof. Andreas Geiger for the helpful discussions. Finally, I want to
  express my very profound gratitude to my parents for their continuous support
  and encouragement throughout my entire life.

 
@@latex:\clearpage@@ 

#+begin_export latex
  \clearpage \tableofcontents \clearpage
#+end_export

* Introduction
  #+BEGIN_COMMENT
  Motivate and abstractly describe the problem you are addressing and how you
  are addressing it. What is the problem? Why is it important? What is your
  basic approach? A short discussion of how it fits into related work in the
  area is also desirable. Summarize the basic results and conclusions that you
  will present.

  #+END_COMMENT

# !What is Autonomous Driving? What is the problem?

Autonomous vehicles are a promising technical solution to important problems in
transportation. Every year more than a million people die due to traffic accidents
[1] primarily caused by human error [2]. Automating driving has the potential to
drastically reduce these accidents. Additionally, self-driving cars could improve
the mobility of people who are not able to drive themselves. The use of supervised
machine learning has become the dominant approach to autonomous driving because
it can handle high-dimensional sensor data such as images well. To train machine
learning algorithms in an end-to-end fashion, meaning directly optimizing a neural
network to perform the full driving task, one needs demonstrations from an expert
driver. Industrial research often collects data from human expert drivers, but this
approach is expensive in terms of money and time.
Simulations [3, 4] are frequently used to perform research on autonomous driving
because new ideas can safely be tested in them. In simulations, an alternative to
human experts called privileged experts is available to perform the data collection
task. Privileged experts are computer programs that have direct access to the
simulator (e.g. knowing the positions of all cars), circumventing the challenging
perception task. These privileged experts can generate labeled data faster than
human experts and at basically no cost.


Deep policy learning makes promising progress to many visuomotor control tasks
ranging from robotic manipula- tion [20, 22, 25, 39] to autonomous driving [4,
47]. By learning to map visual observation directly to control action through a
deep neural network, it mitigates the manual design of controller, lowers the
system complexity, and improves generalization ability. However, the sample ef-
ficiency of the underlying algorithms such as reinforcement learning or
imitation learning remains low. It requires a significant amount of online
interactions or expert demon- strations in the training environment thus limits
its real- world applications.  Many recent works use unsupervised learning and
data augmentation to improve the sample efficiency by pre- training the neural
representations before policy learning.  However, the augmented data in
pretraining such as frames with random background videos [16, 17, 43] shifts
drasti-cally from the original data distribution, which degrades the overall
performance of the model. Also, it remains challenging to generalize the learned
weights to the real- world environment as it is hard to design augmentations
that reflect the real-world diversity. In this work, we explore pretraining the
neural representation on a massive amount of real-world data directly. Figure 1
shows some uncurated YouTube videos, which contain driving scenes all over the
world with diverse conditions such as different weathers, urban and rural
environments, and various traffic densities.  We show that exploiting such
real-world data in deep policy learning can substantially improve the
generalization ability of the pretrained models and benefit downstream tasks
across various domains.


# !What is SSL?
Self-supervised learning (SSL) is an approach of machine learning to learn from
unlabeled data. In this learning approach, the learning happens in two
steps. First, the task is solved based on pseudo-labels which helps to
initialize the network weights. Second, the network is finetuned with ground
truth data.
Self-supervised learning (SSL) is a method of machine learning. It learns from unlabeled sample data. It can be regarded as an intermediate form between supervised and unsupervised learning. It is based on an artificial neural network.[1] The neural network learns in two steps. First, the task is solved based on pseudo-labels which help to initialize the network weights.[2][3] Second, the actual task is performed with supervised or unsupervised learning.[4][5][6] Self-supervised learning has produced promising results in recent years and has found practical application in audio processing and is being used by Facebook and others for speech recognition.[7] The primary appeal of SSL is that training can occur with data of lower quality, rather than improving ultimate outcomes. Self-supervised learning more closely imitates the way humans learn to classify objects.[8]


# !Why SSL in Autonomous Driving?
Learning representations from unlabeled data is a popular topic in visual
recognition. The learned representations are shown to be generalizable across
visual tasks ranging from image classification, semantic segmentation, to object
detection [5, 13, 18, 40, 42]. However, these methods are primarily built for
learning features for recognition tasks rather than control, where an agent acts
in an uncertain environment. Decision-making may not benefit from some visual
information, such as the abstraction of appearance and texture. For instance,
visual factors like lighting and weather may even become confounders in policy
learning, decreasing overall performance [46]. These factors are typically
unimportant to the driving job. The properties that are relevant to the output
action, on the other hand, must be understood.
On the contrary, it is crucial
to learn the features that matter to the output action. For example, at the
driving junction, the traffic light occupies only a few pixels in the visual
observation but has a significant impact on the driver’s actions.

# !What is the goal of this project?
In this work, we explore an existing self-supervised learning approach catered
to our autonomous driving framework. Moreover, we propose a new self-supervised
learning method that outperforms the prior work on the challenging NEAT
validation routes \cite{Chitta2021}. +Also, we propose a data cleansing
technique for and stratified sampling in the dataloader for effective training
that improves the driving performance tremendously+.

#+BEGIN_COMMENT
ACO
===
In this paper, we propose a novel action-conditioned policy pretraining
paradigm, which captures important fea- tures in the neural representation
relevant to the decision- making and benefits downstream tasks. As shown in
Figure 1, we first collect a large corpus of driving videos with a wide range of
weather conditions, from wet to sunny, from all across the world without
labeling. We then generate action pseudo labels for each frame using a
pretrained inverse dynamics model. We finally develop a novel policy pretraining
method called Action-conditioned COntrastive Learning (ACO) that incorporates
the action information in the representation learning. Specifically, instead of
contrasting images based on different augmented views [18], we define a new
contrastive pair conditioned on action similarity. By learning with those
action-conditioned contrastive pairs, the representation captures policy-related
elements that are highly correlated to the actions.  We evaluate the
effectiveness of the action-conditioned pretraining for a variety of tasks, such
as policy learn- ing through Imitation Learning (IL) and Reinforcement Learning
(RL) in end-to-end autonomous driving, and Lane Detection (LD). The experimental
results show that ACO successfully learns generalizable features for the down-
stream tasks.  Our contributions are summarized below:

1. We propose a new paradigm of action-conditioned
policy pretraining on a massive amount of real-world
driving videos.
2. We develop a novel action-conditioned contrastive
learning approach ACO to learn action-related fea-
tures.
3. Comparison of various pretraining methods on down-
stream policy learning tasks shows the feature re-
sulting from the proposed method achieves sufficient
performance gain in the driving tasks.
#+END_COMMENT



# !What are the contributions of this project?
In summary, the main contributions of this work are:
- +We present a novel self-supervised learning approach for autonomous driving.+
- +We propose a new data cleaning pipeline and stratified sampling in training.
- We show how the /inertia problem/ can be addressed by data cleaning strategy.

# !Orgnaziation of the rest of the thesis:
We organize the structure of the thesis as follows. We first provide an overview
of the learning-based pipeline in SAP and its limitations in Section [[related_work]]. We
then introduce details of our methodology in Section [[problem_setting]], followed by the
description of the baseline methods and the effectiveness of our proposed model
compared to the baselines both quantitatively and qualitatively in Section
[[results]].
 
    
** COMMENT Content
 Self-supervised learning (SSL) is a method of machine learning. It learns from
unlabeled sample data. It can be regarded as an intermediate form between
supervised and unsupervised learning. It is based on an artificial neural
network.  [1] The neural network learns in two steps. First, the task is solved
based on pseudo-labels which help to initialize the network weights.[2][3]
Second, the actual task is performed with supervised or unsupervised
learning.[4][5][6] Self-supervised learning has produced promising results in
recent years and has found practical application in audio processing and is
being used by Facebook and others for speech recognition.  [7] The primary
appeal of SSL is that training can occur with data of lower quality, rather than
improving ultimate outcomes. Self-supervised learning more closely imitates the
way humans learn to classify objects.[8]

For a binary classification task, training data can be divided into positive
examples and negative examples.  Positive examples are those that match the
target. For example, if you're learning to identify birds, the positive training
data are those pictures that contain birds. Negative examples are those that do
not.[9] Contrastive SSL uses both positive and negative examples. Contrastive
learning's loss function minimizes the distance between positive samples while
maximizing the distance between negative samples.[9] Non-contrastive SSL uses
only positive examples. Counterintuitively, NCSSL converges on a useful local
minimum rather than reaching a trivial solution, with zero loss. For the example
of binary classification, it would trivially learn to classify each example as
positive. Effective NCSSL requires an extra predictor on the online side that
does not back-propagate on the target side.[9]

SSL belongs to supervised learning methods insofar as the goal is to generate a
classified output from the input.  At the same time, however, it does not
require the explicit use of labeled input-output pairs. Instead, correlations,
metadata embedded in the data, or domain knowledge present in the input are
implicitly and autonomously extracted from the data.[10] These supervisory
signals, generated from the data, can then be used for training.[8] SSL is
similar to unsupervised learning in that it does not require labels in the
sample data. Unlike unsupervised learning, however, learning is not done using
inherent data structures.[10] Semi-supervised learning combines supervised and
unsupervised learning, requiring only a small portion of the learning data be
labeled.[3] In transfer learning a model designed for one task is reused on a
different task.[11]

* Related Work <<related_work>>
 #+BEGIN_COMMENT
 Answer the following questions for each piece of related work that addresses the same or a similar problem. What is their problem and method? How is your problem and method different? Why is your problem and method better?
 #+END_COMMENT

** Self-supervised Learning

*** Emerging Properties in Self-Supervised Vision Transformers

*** Exploring Simple Siamese Representation Learning

*** Momentum Contrast for Unsupervised Visual Representation Learning

*** Self-training with Noisy Student improves ImageNet classification
** Self-supervised Learning in Autonomous Driving
  
*** SelfD: Self-Learning Large-Scale Driving Policies From the Web

**** COMMENT Content

*** Offline Visual Representation Learning for Embodied Navigation

**** COMMENT Content
*** Action-Conditioned Contrastive Policy Pretraining

**** COMMENT Content
* Problem Setting <<problem_setting>>
#+BEGIN_COMMENT
2.1 Task Definition

Precisely define the problem you are addressing (i.e. formally specify the inputs and outputs). Elaborate on why this is an interesting and important problem.

2.2 Algorithm Definition

Describe in reasonable detail the algorithm you are using to address this problem. A psuedocode description of the algorithm you are using is frequently useful. Trace through a concrete example, showing how your algorithm processes this example. The example should be complex enough to illustrate all of the important aspects of the problem but simple enough to be easily understood. If possible, an intuitively meaningful example is better than one with meaningless symbols.

#+END_COMMENT

# What is our goal?
# As an overview, what are we doing to meet our goal?
Our goal is to facilitate training driving policies at scale.

#+BEGIN_COMMENT
SelfD
=====
Our goal is to facilitate training driving policies at scale.  To efficiently
make use of the broad and diverse experience found in large amounts of unlabeled
videos, we follow three main steps. We first propose to use a monocular image-
based behavior cloning planner that reasons directly in the BEV (Sec. 3.3). Our
proposed planner can therefore bet- ter generalize across arbitrary
perspectives. Next, we intro- duce a data augmentation step for obtaining
multiple plausi- ble pseudo-labels when self-training over unlabeled internet
data (Sec. 3.4). Finally, we re-train the model over the larger dataset to learn
a more robust and generalized vision-based navigation policy (Sec. 3.5).


#+END_COMMENT


** Problem Setting <<problem_setting>>
  We consider the task of point-to-point navigation in an urban setting where
  the goal is to complete a given route while safely reacting to other dynamic
  agents and following traffic rules. To achieve this, we consider the imitation
  learning approach of learning policy as in self-driving it is easier for an
  expert to demonstrate the desired behaviour rather than to specify a reward
  function.

  *Imitation Learning (IL):* The goal of IL is, for an agent to learn a policy
   \(\pi_{\v{\th}}\), that imitates the behavior of an expert \(\pi^{*}\). The agent
   learns to map an input to a navigational decision. In general, the decision
   may either be a low-level vehicle control action \cite{Codevilla2019}
   (e.g. steering, throttle and break) or a desired future trajectory relative
   to the ego-vehicle, i.e., a set of \(K\) waypoints
   \cite{Chen2019,Mueller2018} in the BEV space. In the latter case, future
   waypoints may be paired with a hand-specified or learned motion controller to
   produce the low-level action \cite{Chen2019,Mueller2018}. In this work, we
   focus on the later representation due to its interpretability and
   genralizability. To find the mapping, we consider the Behavior Cloning (BC)
   approach of IL. An expert policy is first rolled out to collect at each
   time-step, high-dimensional observations of the environment including front
   camera image, ego-vehicle position and orientation, high-level navigational
   command and high-level goal location provided as GPS coordinates etc. From
   these high-dimensional observations, we derive our dataset \(\DD = \set{(\bf
   x_{i}, \bf w_{i})}_{i=1}^{N}\in(\XX,\WW)\) of size \(N\), where the input
   \(\bf x\) is a subset of the observations +(see ?)+ and the corresponding
   expert trajectory \(\bf w\), defined by a set of 2D waypoints relative to the
   coordinate frame of the ego-vehicle in BEV space, i.e., \(\bf
   w=(x_{t},y_{t})_{t=1}^{T}\), are calculated from the ego-vehicle positions
   and orientations from the subsequent frames. Our goal is to find a decision
   making policy i.e. a waypoint prediction function \(\pi_{\v{\th}}:\XX\to\WW\)
   with learnable parameters \(\v{\th}\in\R^{d}\). In BC, the policy
   \(\pi_{\v{\th}}\) is learned by training a neural network in a supervised
   manner using the dataset, \(\DD\), with a loss function, \(\LL\) i.e.

   \[\under{\tx{argmin }}{{\th}} \Ex_{(\bf x,\bf w)\sim \DD} [\LL(\bf
   w,\pi_{\th}(\bf x))].\]

   We use the \(L_{1}\) distance between the predicted trajectory,
   \(\pi_{\v{\th}}(\bf x)\), and the corresponding expert trajectory, \(\bf w\),
   as the loss function. We assume access to an inverse dynamic model
   \cite{10.2307/j.ctt183ph6v}, implemented as a PID controller \(\I\), which
   performs the low-level control, i.e., steer, throttle and brake, provided the
   future trajectory \(\bf w\). The action are dertermined as \(\bf a=\I(\bf w)\).
   
   *Global Planner:* We follow the standard protocol of CARLA 0.9.10 and assume that
   high-level goal locations G are pro- vided as GPS coordinates. Note that
   these goal locations are sparse and can be hundreds of meters apart, as
   opposed to the local waypoints predicted by the policy \(\pi_{\v{\th}}\).
   +For one of the self-supervised learning methods, we will slightly augment
   the output space of this function in Section+ [[wpn]].
   
** Input and Output Parameterization
We note that, even though our collected expert demostrations +and the youtube
driving videos+ are sequential, we do not use temporal data for
training. Contrary to our intuition of getting better generalization in
decision-making from sequential observations, the prior works on IL for
autonomous driving have shown that using observation histories may not lead to
performance gain \cite{Haan2019,Muller2006,Bansal2018,Wang2019a}. Thus, We use a
single time-step input.


Input Representation: Following [45, 23], we convert the
LiDAR point cloud into a 2-bin histogram over a 2D BEV
grid with a fixed resolution. We consider the points within
32m in front of the ego-vehicle and 16m to each of the sides,
thereby encompassing a BEV grid of 32m × 32m. We di-
vide the grid into blocks of 0.125m × 0.125m which results
in a resolution of 256 × 256 pixels. For the histogram, we
discretize the height dimension into 2 bins representing the
points on/below and above the ground plane. This results in
a two-channel pseudo-image of size 256 × 256 pixels. For
the RGB input, we consider the front camera with a FOV
of 100◦ . We extract the front image at a resolution of 400
× 300 pixels which we crop to 256 × 256 to remove radial
distortion at the edges.
Output Representation: We predict the future trajectory
W of the ego-vehicle in BEV space, centered at the current
coordinate frame of the ego-vehicle. The trajectory is repre-
sented by a sequence of 2D waypoints, {wt = (xt , yt )}Tt=1 .
We use T = 4, which is the default number of waypoints
required by our inverse dynamics model.

** Waypoint Prediction Network <<wpn>>
As shown in +Fig. 2+, we pass the 512-dimensional feature vector through an MLP
(comprising 2 hidden layers with 256 and 128 units) to reduce its dimensionality
to 64 for computational efficiency before passing it to the auto-regressive
waypoint network implemented using GRUs \cite{Cho2014}. We initialize the hidden
state of the GRU with the 64-dimensional feature vector. The update gate of the
GRU controls the flow of information encoded in the hidden state to the output
and the next time-step. It also takes in the current position and the goal
location (Section [[problem_setting]]) as input, which allows the network to focus
on the relevant context in the hidden state for predicting the next waypoint.
We provide the GPS coordinates of the goal location (transformed to the
ego-vehicle coordinate frame) as input to the GRU rather than the encoder since
it lies in the same BEV space as the predicted waypoints and correlates better
with them compared to representing the goal location in the perspective image
domain \cite{Chen2019}. Following \cite{Filos2020}, we use a single layer GRU
followed by a linear layer which takes in the hid- den state and predicts the
differential ego-vehicle waypoints \(\set{\d \bf w_{t}}_{t=1}^{T}\) for \(T=4\)
future time-steps in the ego-vehicle current coordinate frame. Therefore, the
predicted future waypoints are given by \(\set{\bf w_{t}=\bf w_{t-1} +\d \bf
w_{t}}_{t=1}^{T}\). The input to the first GRU unit is given as \((0,0)\) since
the BEV space is centered at the ego-vehicle’s position.

*Controller:* We use two PID controllers for lateral and lon- gitudinal control
to obtain steer, throttle and brake values from the predicted waypoints,
\(\set{\bf w_{t}}_{t=1}^{T}\). The longitudinal controller takes in the
magnitude of a weighted average of the vectors between waypoints of consecutive
time-steps whereas the lateral controller takes in their orientation. For the
PID controllers, we use the same configuration as in the author-provided
codebase of \cite{Chen2019}. Implementation de- tails can be found in the supplementary.
** Loss Function
Following [8], we train the network using an L1 loss be-
tween the predicted waypoints and the ground truth way-
points (from the expert), registered to the current coordi-
nate frame. Let wtgt represent the ground truth waypoint for time-step t, then the loss function is given by:
L=
T
X
||wt − wtgt ||1
(5)
t=1
Note that the ground truth waypoints {wtgt } which are avail-
able only at training time are different from the sparse goal
locations G provided at both training and test time.

** Task
We consider the task of navigation along a set of
predefined routes in a variety of areas, e.g. freeways, urban
areas, and residential districts. The routes are defined by a
sequence of sparse goal locations in GPS coordinates pro-
vided by a global planner and the corresponding discrete
navigational commands, e.g. follow lane, turn left/right,
change lane. Our approach uses only the sparse GPS lo-
cations to drive. Each route consists of several scenarios,
initialized at predefined positions, which test the ability of
the agent to handle different kinds of adversarial situations,
e.g. obstacle avoidance, unprotected turns at intersections,
vehicles running red lights, and pedestrians emerging from
occluded regions to cross the road at random locations. The
agent needs to complete the route within a specified time
limit while following traffic regulations and coping with
high densities of dynamic agents.




We consider the task of driving in an urban setting in the realistic 3D simulator
CARLA version 0.9.13. We use routes that cover both highway and residential
areas. Agents are supposed to follow routes which are provided as GPS waypoints
by an A^* navigational planner. The goal is to arrive at the target location in a given
amount of time while incurring as little infraction penalties as possible. Infractions
that are penalized are:
- Collision with a pedestrian
- Collision with a vehicle
- Collision with a static object
- Running a red light
- Running a stop sign
- Driving on the wrong lane or sidewalk
- Leaving the route specified by the navigational planner
Along the routes, there are dangerous scenarios specified which the agent needs to
resolve in order to safely arrive at his destination. There are currently 10 different
scenarios specified:
- Rubble on the road leading to a loss of control.
- Leading vehicle suddenly performs an emergency brake.
- A pedestrian hidden behind a static object suddenly runs across the street.
- After performing a turn at an intersection, a cyclist suddenly drives across
  the street.
- A slow vehicle gets spawned in front of the agent.
- A static object is blocking the street.
- Crossing traffic is running a red light at an intersection
- The agent must perform an unprotected left turn at an intersection with oncoming traffic
- The agent must perform a right turn at an intersection with crossing traffic
- Crossing an unsignalized intersection.

* Self-supervised Learning Approaches
In this section we will introduce our proposed method and also explore some
prior works in our self-driving framework.

** Learning with Pseudolabels
Contrary to SelfD approach, we do not need to rely on the model's understanding
of driving for pseudo-labels
*** Deep Visual Odometry
In this section, we will introduce our proposed method for self-supervised
learning for autonomous driving. Our key idea is to generate pseudo-labels for
unlabeled driving scenes by exploiting a learning-based ego-motion estimation
method because of its desirable properties of robustness to image noise and
camera calibration independence. Moreover, in our expert driving dataset the
weather conditions and the time of the day changes from frame to frame. For this
reason learning-based method is the best way to estimate ego-motions than
classical methods. We use an end-to-end learning approach following
\cite{Wang2017, Zhai2019} to train the model to map directly from input image
pairs to an estimate of ego-motion (in our use case, estimate of relative
translation is sufficient). The model we used, is a two module Long-term
Recurrent Convolutional Neural Networks.

*Feature-encoding Module:* In order to learn the geometric relationships from
two adjacent images, we use the following CNN architechture, inspired by
FlowNetSimple architechture \cite{Fischer2015} ignoring the decoder part of it
and only focusing the on the convolutional encoder.

\input{table/table1}

Contrary to DeepVO \cite{Wang2017} and PoseConvGRU \cite{Zhai2019}, which uses
huge 10-layered CNN architechtures, we use a very simple and lightweight
5-layered CNN architechture as shown in Table \ref{table1}. Each layer is
followed by an application of ReLU nonlinear activation function. We keep the
kernel size to 3, padding size to 1 for all the layers. The channel dimension
doubles in each subsequent layers. We use maxpool of size 2 in the first 2
layers and use maxpool of size 4 for the rest of the layers. The reason behind
this is that, having small receptive field in the first 2 layers encourages the
network to learn about the fine-grained geometric details in the pair of images
which is essential for relative motion estimation. On the other hand, the large
receptive field in the later layers enforces the network to ignore the global
context and also helps in reducing the number of learnable parameters as
well. The input to the CNN are a sequence of \(n+1\), \(256 \x 256\) RGB driving
scenes from CARLA. With \(n+1\) sequential driving scenes, we can obtain \(n\)
sets of image pairs taking two adjacent frames at a time. These image pairs are
then fed to the 5-layered CNN to obtain a feature maps of size \(1 \x 1 \x
1024\) for each image pair. Contrary to the typical training process with
augmented data for CNNs, we only use the original images for accurate relative
motion estimation, because performing any pre-processing operation to the images
such as blurring, adding noise, random clipping etc., can hamper the network to
learn the geometric relationship of the objects in the images.

*Memory-propagating Module:* Following \cite{Zhai2019}, we use a stacked ConvGRU
(convolutional gated recurrent unit) \cite{Ballas2015} as our memory-propagating
module as showns in +Fig.+. The memory module builds a set of chronological
visual representations from the CNN embeddedings of the sequence of image
pairs. Because of its ability of remember histories, ConvGRU can capture the
geometric relationships coming from the previous frames of images, and then
estimate the relative motion for the current frame utilizing the geometric
constraint within multiple frames. Also, ConvGRUs are appropriate for this
module as they are simple yet powerful. They contain fewer gates compared to
ConvLSTMs and thus reduce the number of learnable parameters and also provide
similar performance as ConvLSTMs \cite{Chung2014}. In our implementation, we
flatten the \(1 \x 1 \x 1024\)-dimensional CNN embeddeding into a feature
vector, which we then further process through a fully connected layer and reduce
its dimensions to 256 for computational efficiency before passing it to the
ConvGRU. Following \cite{Filos2020}, we use a single layer ConvGRU followed by a
linear layer which takes in the hidden state and predicts the relative
translation of the ego-vehicle implied by the pair of images. Finally, we
accumulate all the relative translations to compute the ego-vehicle trajectory.

+should include the math?+
+plot pseudowaypoints+

The feature-encoding module
encodes the short-term motion feature in an image pair, while the memory-
propagating module captures the long-term motion feature in the consecutive
image pairs. The visual memory is implemented with convolutional gated re-
current units, which allows propagating information over time. At each time
step, two consecutive RGB images are stacked together to form a 6 channels
tensor for module-1 to learn how to extract motion information and estimate
poses. The sequence of output maps is then passed through a stacked ConvGRU module to generate the relative transformation pose of each image pair. We also
augment the training data by randomly skipping frames to simulate the veloc-
ity variation which results in a better performance in turning and high-velocity
situations. Randomly horizontal flipping and temporal flipping of the sequences
is also performed. We evaluate the performance of our proposed approach on
the KITTI Visual Odometry benchmark. The experiments show a competitive
performance of the proposed method to the geometric method and encourage
further exploration of learning based methods for the purpose of estimating
camera ego-motion even though geometrical methods demonstrate promising
results.


Self-supervised learning aims at gathering more labels to unlabeled inputs so
that we can generate better representation of the input. In self-driving task
our output is the waypoints. Which can be estimated completely independent of
the driving task and we can generate accurate estimation of the waypoints.

There are two ways to generate better representations of the inputs
1. Contrastive learning
   This kind of learning try to learn a representation that describe the image
   as a whole. Does not find the representation that is actually important to
   the downstream task.
2. by generating pseudolabels
   

it also predicts
accurate ego-motion estimation, 
*** SelfD
    In this section we will discuss the semi-supervised approach ``SelfD''
    by Zhang et al. \cite{Zhang2022a}.
**** Conditional Imitation Learning from Observations
To make use of the unlabeled driving scenes containing diverse navigational
experience, the authors propose `Conditional Imitation Learning from
Observations' (CILfO) \cite{Zhang2022a} framework. The framework suggests a way
to generate pseudo-labels for the unlabeled driving scenes. We explore the
framework in our autonomous driving setting for self-supervised learning. We
consider two conditional commands i.e. navigational command and target point
separately in our experiments +see?+. Thus, with this method we recover the
waypoints \(\hat{\bf w}\), the conditional command \(\hat{c}\) and speed
\(\hat{v}\) to construct a dataset \[\hat{\DD}=\set{(\bf I_{i}, \hat v_{i}, \hat
c_{i}), \bf w_{i}}_{i=1}^{M}\] which can be then use to train a policy using
behaviour cloning.
**** Initial Data Assumption
The CILfO learning task assumes access to a small labeled dataset to learn an
initial policy mapping using human expert demonstrations. We then use this
learned policy to gather pseduo-labels for the unlabeled data. This assumption
is reasonable considering that there are several publicly available driving
datasets with included action labels \cite{Geiger2012AreWR,Argoverse2}. In this
work, we use a small subset of a labeled dataset collected from an expert policy
roll out in CARLA \cite{Dosovitskiy2017}.

**** Self-supervised Training Process
In this section, we discuss the proposed generalized training method for
leveraging unconstrained and unlabeled demonstration data. The proposed
semi-supervised policy training process, SelfD, can be learned in three
summarized steps:
    1. Use a small, labeled domain-specific dataset \(\DD\) to learn an initial
       observations-to-BEV policy \(\pi_{\v{\th}}\) via imitation.
    2. Obtain a large pseudo-labeled dataset \(\hat{\DD}\) by leveraging
       sampling from \(\pi_{\v{\th}}\).
    3. Pre-train a generalized policy \(\pi_{\v{\th}}\) on \(\hat{\DD}\) and
       fine-tune on the clean labels of \(\DD\).

Note that we re-use the parameter symbol \(\v{\th}\) throughout
the steps to simplify notation. @@latex:\hl{Our iterative semi-supervised
training enables effectively augmenting the knowledge and
robustness of an initially trained policy. As described
next, our proposed initial step facilitates learning a platform
and perspective-agnostic policy during subsequent training
steps by directly reasoning in a BEV planning space.}@@ 

3.3. BEV Plan Network
In this section, we propose a suitable output represen-
tation to account for arbitrary cameras, viewpoints and
scene layouts. Current monocular planners generally pre-
dict waypoints in the image plane to align with an input
image [13, 49]. The waypoints are then transformed to a
BEV plan using carefully calibrated camera intrinsic and
extrinsic (e.g., rotation, height) parameters [13]. Thus, pol-
icy models are often trained and evaluated within a fixed
pre-assumed setup. In contrast, SelfD predicts a future plan
parameterized by waypoints in the BEV plan space directly.
Based on our experiments in Sec. 4, we demonstrate this
choice to be crucial for real-world planning across settings.
The predicted generalized BEV waypoints can be paired
with a low-level controller, e.g., a PID controller [13, 49].
Due to the difficulty in learning a monocular-to-BEV plan
mapping, we follow recent work in confidence-aware learn-
ing [38] to train an augmented model fθ : X → Y × R
with quality estimates σ ∈ R. Our training loss function in
Eqn. 1 is defined as
L = Lplan + λLquality
(3)
where Lplan is the L1 distance between ground-truth and
predicted waypoints, Lquality is a binary cross-entropy
loss [38, 78], and the λ hyper-parameter balances the tasks.
3.4. “What If” Pseudo-Labeling of Unlabeled Data
Given a set of unlabeled images U, we sample from the
trained conditional policy fθ in a semi-supervised training
process. While the speed and command inputs to fθ can be
recovered through visual odometry techniques [73], these
result in highly noisy trajectories in our online video set-
tings (discussed in Sec. 4). As the demonstrations in our
data may be unsafe or difficult to recover, we propose to
leverage a single-frame pseudo-labeling mechanism. Our
key insight is to employ the conditional model fθ to gener-
ate multiple hypothetical future trajectories in a process re-
ferred to as “what if” augmentation. Beyond resolving the
missing speed and command inputs, our proposed augmen-
tation provides additional supervision, i.e., a conditional
agent that better reasons on what it might need to do, for in-
stance, if it had to turn left instead of right at an intersection
(Fig. 2). In contrast to related work in policy learning and
distillation [13], sampling from the teacher agent is more
challenging as the agent is not exposed to extensive 3D per-
ception knowledge about the world and is being evaluated
outside of its training settings.
We repeatedly sample v̂ and ĉ uniformly and rely on
the conditional model to provide pseudo-labels (ŷ, σ̂) =
fθ (I, v̂, ĉ) for additional supervision across all conditional
branches and speed observations. In this manner, querying
the “teacher” model fθ enables us to generate various sce-
narios beyond the original demonstration. In particular, as
discussed in Sec. 4, we find self-training strategies to pro-
vide limited generalization gains without this “what if” data
augmentation step. This augmentation strategy enables our
single-frame pseudo-labeling approach to significantly out-
perform approaches that are more elaborate to train at scale,
as they may involve additional modules relying on approxi-
mating ŷ, ĉ, and v̂ from video. Finally, to avoid incorporat-
ing potentially noisy trajectories, the corresponding quality
estimates σ̂ can be used to process and filter examples in
the pseudo-labeled dataset D̂.
3.5. Model Pre-Training and Fine-Tuning
As a final training step, we re-train the waypoint network
fθ from scratch over the large and diverse dataset D̂. The
pre-trained policy can then be further fine-tuned over the
original dataset D, thus leveraging the additional knowledge
gained from D̂ to improve its performance. We note that
we employ separate training over the two datasets D̂ and D
and rely on knowledge transfer through learned represen-
tations as it reduces the need for any careful hyperparam-
eter tuning beyond the overall learning rate. For instance,
Caine et al. [8] empirically demonstrate the importance of
delicately optimizing the ratio of labeled to pseudo-labeled
data when mixing the datasets for a 3D object detection
task, while also showing it to vary among object categories,
e.g., pedestrians vs. vehicles. Thus, through a pre-training
mechanism, we avoid the need to carefully mix the cleanly
labeled and pseudo-labeled datasets [8, 65, 78].
3.6. Implementation Details
Speed: 0.24 Speed: 16.21 Speed: 2.84
Score: 0.6 Score: 0.09 Score: 0.42
Cmd: 1
Cmd: 2
Cmd: 3
We implement our BEV waypoint prediction network
fθ leveraging a state-of-the-art conditional sensorimotor
agent [13]. However, as discussed in Sec. 3.2, we do not
assume a fixed known BEV perspective transform. Thus,
we remove the fixed perspective transformation layer which
restricts scalability and replace it with a per-branch BEV
prediction module (see supplementary for additional imple-
mentation and architecture details). During training we use
a learning rate of 1e−3 and resize images to 400 × 225.
** Self-supervised Representation Learning
*** Action Conditioned Policy Pretraining
*** OVRL
In this section, we discuss OVRL\cite{Yadav2022}, a two-stage learning approach
proposed by Yadav et. al. and incorporate it in our autonomous driving
framework. As an overview, this learning approach includes an encoder
pretraining step using DINO, followed by downstream policy learning via behavior
cloning.
**** Self-supervised Pretraining
As the first step, a visual encoder is pretrained using DINO \cite{Caron2021}, a
simple but effective self-supervised learning algorithm. DINO uses knowledge
distillation as a mechanism for self-training, where the student network
\(g_{\vth_{s}}\) is trained to match the output of the teacher network
\(g_{\vth_{t}}\).  As illustrated in +Figure 2(left)+, it takes an input image
\(x\) and using the multi-crop strategy introduced in \cite{Caron2020}, it
generates multiple distorted views or crops from it, specifically, it produces
two global views (\(x_{1}^{g}\) and \(x_{2}^{g}\)) at \(224\x 224\) resolution
and eight local views \(x^{l}\) at a lower resolution (\(96\x 96\)). All crops
are passed through the student network while only the global views are passed
through the teacher network.  The student and teacher networks both output \(K\)
dimensional feature vectors for each view, which are converted into probability
distributions (\(P_{s}\) and \(P_{t}\)) using a temperature scaled softmax
function as follows:
\[P_{s}(x)^{(i)}=\q{\exp\inp{g_{\vth_{s}}(x)^{(i)}/\t_{s}}}{\sum_{k=1}^{K}\exp\inp{g_{\vth_{s}}(x)^{(k)}/\t_{s}}}\]
where the temperature parameter \(\t_{s}\) controls the sharpness of the output
distribution, and a similar formula holds for \(P_{t}\) with temperature
\(\t_{t}\). Now, given a fixed teacher network \(g_{\vth_{t}}\), the student
network learns to match the distribution of the teacher network by minimizing
the cross-entropy loss:
\[\LL(\vth_{s})=\sum_{x\in{x_{1}^{g},x_{2}^{g}}}\sum_{x'\in
\set{x_{1}^{g},x_{2}^{g}}\u\set{x^{l}_{i}}_{i=1}^{8}}P_{t}(x)\log(P_{s}(x')).\] The
teacher network parameters are updated as an exponential moving average of the
student network parameters.

Representation learning by self-supervised learning algorithm is always prone to
collapse, i.e., the network can converge to a trivial solution by predicting the
same representation for every image. To avoid collapse, DINO centers and
sharpens the teacher’s output before the softmax operation. Specifically, the
centering operation adds the term \(c\) to the teacher’s output, which is
updated as follows: \(c\la mc+(1-m)\q1B\sum_{i=0}^{B}g_{\vth_{t}}(x_{i})\),
where \(m>0\) is a momentum parameter and \(B\) is the batch size. Sharpening is
achieved by setting low value for the temperature parameter \(\t_{t}\) for the
teacher softmax normalization. Thus, centering prevents one dimension to
dominate but encourages collapse to the uniform distribution, while the
sharpening has the opposite effect. Applying both operations balances their
effects which is sufficient to avoid collapse in presence of a momentum teacher.

@@latex:\hl{We use the convolutional layers in a modified ResNet50 [21] architecture com-
bined with a projection head for the student and teacher networks. Specifically,
we modify the ResNet50 by 1) reducing the number of output channels at every
layer by half (i.e. using 32 ResNet baseplanes instead of 64) and 2) using Group-
Norm [48] instead of BatchNorm in our backbone, similar to DDPPO [47]. The
projection head is a 3-layer MLP (Multi-Layer Perceptron) with 2048 hidden
units followed by l2 norm and a weight normalized fully connected layer of K
dimension. We keep the BatchNorm in the MLP, however the whole projection
head is discarded after training.}@@ 

**** Downstream Learning


We illustrate DINO in
the case of one single pair of views (x1 , x2 ) for simplicity. The
model passes two different random transformations of an input
image to the student and teacher networks. Both networks have
the same architecture but different parameters. The output of the
teacher network is centered with a mean computed over the batch.
Each networks outputs a K dimensional feature that is normalized
with a temperature softmax over the feature dimension. Their
similarity is then measured with a cross-entropy loss. We apply a
stop-gradient (sg) operator on the teacher to propagate gradients
only through the student. The teacher parameters are updated with
an exponential moving average (ema) of the student parameters.

* Experimental Results <<results>>

#+BEGIN_COMMENT
3.1 Methodology

What are criteria you are using to evaluate your method? What specific hypotheses does your experiment test? Describe the experimental methodology that you used. What are the dependent and independent variables? What is the training/test data that was used, and why is it realistic or interesting? Exactly what performance data did you collect and how are you presenting and analyzing it? Comparisons to competing methods that address the same problem are particularly useful.

3.2 Results

Present the quantitative results of your experiments. Graphical data presentation such as graphs and histograms are frequently better than tables. What are the basic differences revealed in the data. Are they statistically significant?

3.3 Discussion

Is your hypothesis supported? What conclusions do the results support about the strengths and weaknesses of your method compared to other methods? How can the results be explained in terms of the underlying properties of the algorithm and/or the data.
#+END_COMMENT
In this section, we describe our experimental setup, com-
pare the driving performance of our approach against sev-
eral baselines, conduct an infraction analysis to study dif-
ferent failure cases, visualize the attention maps of Trans-
Fuser and present an ablation study to highlight the impor-
tance of different components of our model.
 
** Implementation Details

*** Input and Output Parameterization
Input Representation: Following [45, 23], we convert the
LiDAR point cloud into a 2-bin histogram over a 2D BEV
grid with a fixed resolution. We consider the points within
32m in front of the ego-vehicle and 16m to each of the sides,
thereby encompassing a BEV grid of 32m × 32m. We di-
vide the grid into blocks of 0.125m × 0.125m which results
in a resolution of 256 × 256 pixels. For the histogram, we
discretize the height dimension into 2 bins representing the
points on/below and above the ground plane. This results in
a two-channel pseudo-image of size 256 × 256 pixels. For
the RGB input, we consider the front camera with a FOV
of 100◦ . We extract the front image at a resolution of 400
× 300 pixels which we crop to 256 × 256 to remove radial
distortion at the edges.
Output Representation: We predict the future trajectory
W of the ego-vehicle in BEV space, centered at the current
coordinate frame of the ego-vehicle. The trajectory is repre-
sented by a sequence of 2D waypoints, {wt = (xt , yt )}Tt=1 .
We use T = 4, which is the default number of waypoints
required by our inverse dynamics model.

*** Dataset

*** Data Preprocessing Pipeline <<datapp>>


*** Training and Inference

** Dataset
We use the CARLA \cite{Dosovitskiy2017} simulator for training and testing,
specifically CARLA 0.9.10 which consists of 8 publicly available towns. We use 7
towns for training and hold out Town05 for evaluation. For generating training
data, we roll out an expert policy designed to drive using privileged
information from the simulation and store data at 2 FPS. Please refer to the
supplementary material for additional details. We select Town05 for evaluation
due to the large diversity in drivable regions compared to other CARLA towns,
e.g. multi-lane and single-lane roads, highways and exits, bridges and
underpasses. We consider two evaluation settings: (1) Town05 Short: 10 short
routes of 100-500m comprising 3 intersections each, (2) Town05 Long: 10 long
routes of 1000-2000m comprising 10 intersections each. Each route consists of
a high density of dynamic agents and adversarial scenarios which are spawned
at predefined positions along the route. Since we focus on handling dynamic
agents and adversarial scenarios, we decouple this aspect from generalization
across weather conditions and evaluate only on ClearNoon weather.
** Evaluation Metrics <<metric>> 
For the CARLA Autonomous Driving Leaderboard, the driving performance of an
agent is characterized by a set of chosen metrics that considers different
aspects of driving. While all routes have the same type of metrics, their
respective values are calculated separately. These metrics are as follows:

*** Route Completion
Route Completion is the percentage of the route that is completed by an
agent. If an agent drives off-road, that percentage of the route will not be
considered towards the computation of the route completion score. Additionally,
the following events will interrupt the simulation, preventing the agent to
continue which will effectively reduce the route completion:

- Route deviation: If an agent deviates more than 30 meters from the assigned route.
- Agent blocked: If an agent doesn’t take any actions for 180 simulation seconds.
- Simulation timeout: If no client-server communication can be established in 60 seconds.
- Route timeout: If the simulation of a route takes too long to finish.

*** Infraction Score
Infraction Score is a penalty for infractions where the agent starts with
an ideal base score of 1.0. For every infraction, the score is multiplied by the
penalty coefficient of that infraction type. Ordered by their severity, the
penalty coefficients are as follows:
- Collision with a pedestrian: 0.50
- Collision with a vehicle: 0.60
- Collision with a static object: 0.65
- Running a red light: 0.70
- Running a stop sign: 0.80
Note that this means that subsequent infractions will have a lower impact due to the
multiplicative nature of the score.

*** Driving Score
Driving Score is the main metric for performance, serving as the product between
the route completion and the infractions penalty. It is calculated in the
following way: \[\tx{Driving Score}=\q1N\sum_{i=1}^{N}R_{i}P_{i}\] where \(N\) is the
number of routes, \(R_{i}\) the route completion percentage of the \(i\)-th route and
\(P_{i}\) the infraction penalty of \(i\)-th route. Note that, this is not the
same as multiplying the averaged route completion with the averaged infraction
score. The driving score is a normalized metric, meaning that the best possible
score is 100 and the worst score is 0. For the validation routes, we run them
with 3 different seeds and report the mean and standard deviation of the 3
scores averaged across the routes.

** Comparisons to the Baselines Methods <<complap>>   
** Ablation Studies
  
* Conclusion

#+BEGIN_COMMENT
Briefly summarize the important results and conclusions presented in the paper. What are the most important points illustrated by your work? How will your results improve future research and applications in the area?

#+END_COMMENT
We envision broad and easily deployable autonomous navigation systems. However,
access to resources and data limits the scope of the brittle autonomous systems
today.  Our SelfD approaches enables to significantly improve an initially
trained policy without incurring additional data col- lection or annotation
efforts, i.e., for a new platform, per- spective, use-case, or ambient
settings. Crucially, due to the proposed underlying model architecture, we do
not in- corporate camera parameters or configuration assumptions into the
monocular inference. As SelfD is self-improving, a future direction could be to
continue and learn from increas- ingly larger online datasets beyond what is
described in our study. While we emphasized efficient large-scale training in
our model development, how to best extend SelfD to more explicitly leverage
temporal demonstration data is still an open question which could be studied
further in the future.  Finally, beyond complex 3D navigation, it would be
inter- esting to explore the applicability of our proposed training framework
for learning various embodied tasks from unla- beled web data.

In our experiments we have only used front camera for navigational
decesion. Even though, it can successful in perfect route completion, it
sometimes results in infractions due to invisibilty for example, ego-vehicle
changes lane but the rear vehicle collide with it or the passenger starts
walking after the car already have gone past it.

\printbibliography
